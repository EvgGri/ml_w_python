#  Оценка релевантности слова методом tf-idf (term-frequency - inverse document frequency)
# Когда мы анализируем текстовые данные, мы часто встречаемся со словами в обоих классах (положительный отзыв и отрицательный)
# в двух и более документах. Такие часто встречающиеся слова, как правило, не содержат полезной или отличительной информации.
 # tf-idf - частота термина, обратная частота документа, метод позволяет использоваться для понижающего взвешивания частот
 # часто встречающихся слов в векторах признаков. (определяется, как произведение этих 2-х показателей)

# tf-idf(t,d)=tf(t,d) x idf(t,d)
# При этом tf(t,d)-частота термина, при этом idf(t,d) = log (n_d / (1+df(d,t))), где n_d - общее число документов,
# df(t,d) - число документов d, которые содержат термин t.
# При этом добавление 1 в знаменатель служит для назначения ненулевого значения терминам, которые встречаются во всех
# тренировочных образцах, логарифм используется для того, чтобы низкие частоты документов гарантированно не получали большие веса.

# -=-=-=-=-=-= Прочитаем необходимые данные

# Модель мешка слов на основе частотности слов
# Метод fit_transform создает словарь и преобразовывает 3 элемента в разреженные векторы признаков
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer()
docs = np.array(['The sun is shining', 'The weather is sweet', 'The sun is shining and the weather is sweet, and one and one is too'])
bag = count.fit_transform(docs)
print(count.vocabulary_)
print(bag.toarray())

# -=-=-=-=-=-= Рассмотрим реализацию tf-idf в библиотеке scikit-learn -=-=-=-=-=-=-=

# Существует класс-преобразователь коэффициентов tf-idf, который в качестве входных данных принимает из векторизатора частотностей
# CountVectorizer исходные частоты терминов и преобразовывает их в серию tf-idf'ов:
from sklearn.feature_extraction.text import TfidfTransformer
tfidf=TfidfTransformer()
np.set_printoptions(precision=2)
print(tfidf.fit_transform(count.fit_transform(docs)).toarray())

# Как мы видели ранее, слову is имело наибольшую частоту термина в 3-ем документе, при этом оно было наиболее встречающимся словом.
# Однако, после преобразования того же вектора признаков в tf-idf мы видим, что в документе 3 слову is теперь поставлен в соответствие
# относительно малый tf-idf(0.39), поскольку оно также содержится в документах 1 и 2 и поэтому в ряд ли будет содержать какую-то полезную
# отличительную информацию.

# В scikit-learn вычисление tf-idf(t,d)=tf(t,d) x (idf(t,d) - 1)
# Перед вычислением tf-idf обычно происходит нормализация исходных частот документов, в scikit-learn по умолчанию используется
# L2-регуляризация, которая вектор признаков делит на его L2-норму, возращая вектор длиной 1.

# -=-=-=-=-=-=-= Очистка текстовых данных -=-=-=-=-=-=-=

# Прочитаем базы отзывов о кинофильмах
import numpy as np
import pandas as pd
df=pd.read_csv('./data/movie_data.csv')

df.loc[23,'review'][:50]
# В отзыве содержится лишняя информация, удалим все лишние за исключением символов-эмоций (эмограммы) вроде ':)'
# Для этого воспользуемся библиотекой регулярных выражений Python re
import re
def preprocessor(text):
    text=re.sub('<[^>]*>','', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text)
    text = re.sub('[\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-','')
    return(text)
# <[^>]*>' - в первом регулярном выражении мы попытались убрать всю html-разметку из текста
# после этого мы ищем эмограммы, которые мы временно сохранили, как emoticons
# Затем регулярным выражением [\W+] мы удалили из текста все несловарные символы, преобразовали текст в строчные буквы и добавили временно
# сохраненные emoticons в конец обработанной последовательности символов документа, кроме того, мы удалили из эмограмм символ носа '-'

preprocessor(df.loc[23,'review'][:50])
df['review'] = df['review'].apply(preprocessor)

# -=-=-=-=-=-=-= Переработка документов в лексеммы -=-=-=-=-=-=-=
# Как разделить текстовый корпус на отдельные элементы? - Один из способов, разделить на слова по пробельым символам.
def tokenizer(text):
    return text.split()

tokenizer("Example of the splitting")
# В контексте лексемизации другой метод состоит в выделении основы слова (word stemming) - процесс редукции слова к его основе.
# Оригинальный алгоритм стемминга был разработан Мартином Портером, он используется в библиотеке NLTK
# Более новый алгоритм-стеммер Snowball (eng), стоит отметить, что данные алгоритмы могут создавать и несуществующие слова.
# Но алгоритмы лемматизации стремятся получить канонические формы отдельных слов - леммы.
from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()
def tokenizer_porter(text):
    return [porter.stem(word) for word in text.split()]

tokenizer_porter("Example of the splitting")

# -=-=-=-=-=-=-= Удаление стоп-слов -=-=-=-=-=-=-=
# Стоп-слова: распространенные слова во всех видах текстов, которые несут в себе мало полезной информации.
# Примеры стоп-слов: is, had, and.
# В случае, если мы работаем с исходными или нормализованными частотами терминов, а не tf-idf'ами, которые и без того
# понижают веса часто встречающихся слов, бывает полезно удалить стоп-слова.
# Воспользуемся набором из 127 анлийских стоп-слов, имеющихся в библиотеке NLTK
import nltk
nltk.download('stopwords')
# nltk.download('all')
# Загрузим лингвистический корпус Университета Брауна
from nltk.corpus import brown
brown.words()

from nltk.corpus import stopwords
stop = stopwords.words('english')
[w  for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]

# -=-=-=-=-=-=-= Тренировка лонистической регрессионной модели для задачи классификации документов -=-=-=-=-=-=-=
