#  Оценка релевантности слова методом tf-idf (term-frequency - inverse document frequency)
# Когда мы анализируем текстовые данные, мы часто встречаемся со словами в обоих классах (положительный отзыв и отрицательный)
# в двух и более документах. Такие часто встречающиеся слова, как правило, не содержат полезной или отличительной информации.
 # tf-idf - частота термина, обратная частота документа, метод позволяет использоваться для понижающего взвешивания частот
 # часто встречающихся слов в векторах признаков. (определяется, как произведение этих 2-х показателей)

# tf-idf(t,d)=tf(t,d) x idf(t,d)
# При этом tf(t,d)-частота термина, при этом idf(t,d) = log (n_d / (1+df(d,t))), где n_d - общее число документов,
# df(t,d) - число документов d, которые содержат термин t.
# При этом добавление 1 в знаменатель служит для назначения ненулевого значения терминам, которые встречаются во всех
# тренировочных образцах, логарифм используется для того, чтобы низкие частоты документов гарантированно не получали большие веса.

# -=-=-=-=-=-= Прочитаем необходимые данные

# Модель мешка слов на основе частотности слов
# Метод fit_transform создает словарь и преобразовывает 3 элемента в разреженные векторы признаков
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer()
docs = np.array(['The sun is shining', 'The weather is sweet', 'The sun is shining and the weather is sweet, and one and one is too'])
bag = count.fit_transform(docs)
print(count.vocabulary_)


# -=-=-=-=-=-= Рассмотрим реализацию tf-idf в библиотеке scikit-learn

# Существует класс-преобразователь коэффициентов tf-idf, который в качестве входных данных принимает из векторизатора частотностей
# CountVectorizer исходные частоты терминов и преобразовывает их в серию tf-idf'ов:
