#  Оценка релевантности слова методом tf-idf (term-frequency - inverse document frequency)
# Когда мы анализируем текстовые данные, мы часто встречаемся со словами в обоих классах (положительный отзыв и отрицательный)
# в двух и более документах. Такие часто встречающиеся слова, как правило, не содержат полезной или отличительной информации.
 # tf-idf - частота термина, обратная частота документа, метод позволяет использоваться для понижающего взвешивания частот
 # часто встречающихся слов в векторах признаков. (определяется, как произведение этих 2-х показателей)

# tf-idf(t,d)=tf(t,d) x idf(t,d)
# При этом tf(t,d)-частота термина, при этом idf(t,d) = log (n_d / (1+df(d,t))), где n_d - общее число документов,
# df(t,d) - число документов d, которые содержат термин t.
# При этом добавление 1 в знаменатель служит для назначения ненулевого значения терминам, которые встречаются во всех
# тренировочных образцах, логарифм используется для того, чтобы низкие частоты документов гарантированно не получали большие веса.

# -=-=-=-=-=-= Прочитаем необходимые данные

# Модель мешка слов на основе частотности слов
# Метод fit_transform создает словарь и преобразовывает 3 элемента в разреженные векторы признаков
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer()
docs = np.array(['The sun is shining', 'The weather is sweet', 'The sun is shining and the weather is sweet, and one and one is too'])
bag = count.fit_transform(docs)
print(count.vocabulary_)
print(bag.toarray())

# -=-=-=-=-=-= Рассмотрим реализацию tf-idf в библиотеке scikit-learn -=-=-=-=-=-=-=

# Существует класс-преобразователь коэффициентов tf-idf, который в качестве входных данных принимает из векторизатора частотностей
# CountVectorizer исходные частоты терминов и преобразовывает их в серию tf-idf'ов:
from sklearn.feature_extraction.text import TfidfTransformer
tfidf=TfidfTransformer()
np.set_printoptions(precision=2)
print(tfidf.fit_transform(count.fit_transform(docs)).toarray())

# Как мы видели ранее, слову is имело наибольшую частоту термина в 3-ем документе, при этом оно было наиболее встречающимся словом.
# Однако, после преобразования того же вектора признаков в tf-idf мы видим, что в документе 3 слову is теперь поставлен в соответствие
# относительно малый tf-idf(0.39), поскольку оно также содержится в документах 1 и 2 и поэтому в ряд ли будет содержать какую-то полезную
# отличительную информацию.

# В scikit-learn вычисление tf-idf(t,d)=tf(t,d) x (idf(t,d) - 1)
# Перед вычислением tf-idf обычно происходит нормализация исходных частот документов, в scikit-learn по умолчанию используется
# L2-регуляризация, которая вектор признаков делит на его L2-норму, возращая вектор длиной 1.

# -=-=-=-=-=-=-= Очистка текстовых данных -=-=-=-=-=-=-=

# Прочитаем базы отзывов о кинофильмах
import numpy as np
import pandas as pd
df=pd.read_csv('./data/movie_data.csv')

df.loc[23,'review'][:50]
# В отзыве содержится лишняя информация, удалим все лишние за исключением символов-эмоций (эмограммы) вроде ':)'
# Для этого воспользуемся библиотекой регулярных выражений Python re
import re
def preprocessor(text):
    text=re.sub('<[^>]*>','', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text)
    text = re.sub('[\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-','')
    return(text)
# <[^>]*>' - в первом регулярном выражении мы попытались убрать всю html-разметку из текста
# после этого мы ищем эмограммы, которые мы временно сохранили, как emoticons
# Затем регулярным выражением [\W+] мы удалили из текста все несловарные символы, преобразовали текст в строчные буквы и добавили временно
# сохраненные emoticons в конец обработанной последовательности символов документа, кроме того, мы удалили из эмограмм символ носа '-'

preprocessor(df.loc[23,'review'][:50])
