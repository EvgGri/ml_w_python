# --Decision tree / деревья решений

# Используя алгоритм выбора решения, мы начинаем в корне дерева и расщепляем данные по признаку, который ведет к самому большому
# приросту информации (Information Gain).
# Далее процедура расщепления повторяется, пока мы не получим однородных листов. В силу этого дерево обычно обрезается путем
# установления его максимальной глубины.

# Максимизация прироста информации
# Для того, чтобы расщепить узлы в самых информативных узлах, нам необходимо выбрать целевую функцию, которую мы хотим оптимизировать.

# Функция прироста информации:
# IG(D_p,f) = I(D_p) - sum[(N_j/N_p)*I(D_j)] , здесь суммирование идёт по индексу j
# f - признак по которому выполняется расщепление
# D_p, D_j - родительский и дочерний узлы
# I - мера неоднородности
# N_p - общее число образцов в родительском узле
# N_j - общее число образцов в j-ом дочернем узле
# Таким образом, функция прироста информации - это разница между неоднородностью в родительском узле и сумме неоднородностей в дочернем узле.

# Вместе с тем, чтобы уменьшить комбинаторное пространство поиска, в Scikit-learn реализованы бинарные деревья решений
# Родительский узел расщепляется на 2 D_left & D_right:
# IG(D_p,f) = I(D_p) - (N_left/N_p)*I(D_left) - (N_right/N_p)*I(D_right)

# В силу вышесказанного в бинарных деревьях решений обычно используется 3 меры неоднородности или критерия расщепления:
# 1. мера неоднородночти Джини I_g
# 2. энтропия I_h
# 3. ошибка классификации I_e

import matplotlib. pyplot import numpy as np
def gini(p):
    return (p)*(1-(p))+(1-p)*(1-(1-p))

def entropy(p):
    return -p*np.log2(p) - (1-p)*np.log2(1-p)

as plt
+ ( 1 - p ) * ( l -
- ( 1 -     * n p . l o g 2 ( ( 1 -
def error
return 1 - np.max([p, 1 -
  = np.arange      1.      1)
ent=[entropy(p) if  !=  elseNone for  in   sc_ent =         if   else None for   in ent]
err= [error(i)foriin
fig =plt.figure()
     plt.subplot(lll)
