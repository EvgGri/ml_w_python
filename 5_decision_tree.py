# --Decision tree / деревья решений

# Используя алгоритм выбора решения, мы начинаем в корне дерева и расщепляем данные по признаку, который ведет к самому большому
# приросту информации (Information Gain).
# Далее процедура расщепления повторяется, пока мы не получим однородных листов. В силу этого дерево обычно обрезается путем
# установления его максимальной глубины.

# Максимизация прироста информации
# Для того, чтобы расщепить узлы в самых информативных узлах, нам необходимо выбрать целевую функцию, которую мы хотим оптимизировать.

# Функция прироста информации:
# IG(D_p,f) = I(D_p) - sum[(N_j/N_p)*I(D_j)] , здесь суммирование идёт по индексу j
# f - признак по которому выполняется расщепление
# D_p, D_j - родительский и дочерний узлы
# I - мера неоднородности
# N_p - общее число образцов в родительском узле
# N_j - общее число образцов в j-ом дочернем узле
# Таким образом, функция прироста информации - это разница между неоднородностью в родительском узле и сумме неоднородностей в дочернем узле.

# Вместе с тем, чтобы уменьшить комбинаторное пространство поиска, в Scikit-learn реализованы бинарные деревья решений
# Родительский узел расщепляется на 2 D_left & D_right:
# IG(D_p,f) = I(D_p) - (N_left/N_p)*I(D_left) - (N_right/N_p)*I(D_right)

# В силу вышесказанного в бинарных деревьях решений обычно используется 3 меры неоднородности или критерия расщепления:
# 1. мера неоднородночти Джини I_g
# 2. энтропия I_h
# 3. ошибка классификации I_e

import matplotlib.pyplot as plt
import numpy as np

def gini(p):
    return (p)*(1-(p))+(1-p)*(1-(1-p))

def entropy(p):
    return -p*np.log2(p) - (1-p)*np.log2(1-p)

def error(p):
    return 1 - np.max([p, 1-p])

x = np.arange(0.0, 1.0, 0.01)
ent = [entropy(p) if p != 0 else None for p in x]
sc_ent = [e * 0.5 if e else None for e in ent]
err = [error(i) for i in x]

fig =plt.figure()
ax = plt.subplot(111)

for i , lab, ls, c, in zip([ent, sc_ent, gini(x), err],
                    ['Энтропия', 'Энтропия (шкалированная)',
                    'Неоднородность Джини', 'Ошибка классификации'],
                    ['-', '-', '--', '-.'],
                    ['black', 'lightgray', 'red', 'green', 'cyan']):
    line = ax.plot(x, i, label = lab, linestyle = ls, lw = 2, color = c)
ax.legend(loc='center', bbox_to_anchor=(0.5, 1.15),
          ncol=3, fancybox=True, shadow=False)
ax.axhline(y=0.5, linewidth=1 , color= 'k', linestyle= '--')
ax.axhline(y=1.0, linewidth=1 , color= 'k', linestyle= '--')
plt.ylim([0.0, 1.1])
plt.xlabel('p(i=1)')
plt.ylabel('Индекс неоднородности')
plt.show()
