# Core verctor machine: Fast SVM Training on Very Large Data Sets
# Ядерные машины опорных векторов.

# Классы библиотек Scikit-Learn Perceptron и LogisticRegression, которые были использованы ранее, используют
# высокооптимизированную библиотеку Liblinear на C/C++, аналогично класс SVC для тренировки модели SVM использует
# библиотеку Livsvm, специализированную под методы опорных векторов.

 # Преимущество этих библиотек в том, что  они позволяют достаточно быстро тренировать большое число линейных классификаторов.
 # Иногда наборы данных слишком большие для того, чтобы уместить их в памяти компьютеров. По этим причинам в библиотеке
 # Scikit-Learn существует реализация средствами класса SGDClassifier. Причем этот класс, помимо прочего, поддерживает и
 # динамическое (онлайн) обучение, используя для этого метод partial_fit.

# В основе класса SGDClassifier лежит идея, которая похожа на алгоритм стохастического градиента.
# Инициализировать логистическую регрессию и метод опорных векторов в версии со стохастическим градиентным спуском с параметрами
# по умолчанию можно следующим образом:
#
# from sklearn.linear_model import SGDClassifier
# ppn = SGDClassifier(loss='Perceptron')
# lr = SGDClassifier(loss='log')
# svm = SGDClassifier(loss='hinge')

# Еще одна причина популярности SVM состоит в том, что эти модели легко кернелизировать, т.е. модифицировать с использованием ядра
# для решения нелинейных задач классификации.

# Сгенерируем линейно неразделимый набор данных, который поможет нам (вид логического элемента XOR, используя Numpy) с примером данного
# типа классификации. Используя функцию logical_xor сгенерируем набор данных, где первым 100 элементам назначается класс 1,
# последним 100 элементам назначим класс -1.

# Сделаем так, чтобы у нас появились данные XOR со случайным шумом
# Очевидно, что такие данные не могут быть разделимы линеной гиперплоскостью, используя линеной логистической регресии и
# линейной модели SVM
import numpy as np
np.random.seed(0)
X_xor = np.random.randn(200,2)
y_xor = np.logical_xor(X_xor[:,0] > 0, X_xor[:,1] > 0)
y_xor = np.where(y_xor,1, -1)

import matplotlib.pyplot as plt
plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1 , 1] ,
            c='b', marker='x', label='1')
plt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1],
            c='r', marker='s', label='-1')
plt.ylim(-3.0)
plt.legend()
plt.show()

# Ключевая идея в основе ядерных методов для решения задач с такими линейно неразделимыми данными состоит в том, чтобы создать
# нелинейные комбинации исходных признаков и функцией отображения phi спроецировать их на пространство более высокой размерности,
# где они становятся линейно разделимыми.

# Таким образом, мы можем трансформировать двумерное пространство признаков в трехмерное таким образом, что классы становятся разделимыми.
# phi(x1,x2)=(z1,z2,ż3)=(x1,x2,x1^2+x2^2)
# Это дает нам возможность разделить эти 3 класса линейной гиперплоскостью, которая становится нелинейной границей решения, если
# спроецировать её назад, на исходное пространство признаков. (страница 90, книга "Python и машинное обучение")

# Чтобы решить нелинейную задачу с помощью SVM, мы переводим с помощтью функции phi тренировочные данные в пространство большей размерности,
# и тренируем линейную модель в этом новом пространстве признаков. Далее на новых, ранее не встречавшихся данных мы используем ту же
# функцию phi, чтобы трансформировать новые данные.

# Основная проблема в данном случае - трудозатраты, которые необходимо совершить при конструировании новых признаков, особенно, если тренировочных
# данных очень много.
# Именно здесь помогает "ядерный трюк", который заменяет скалярное произведение на функцию ядра.
# Мы определяем ядерную функцию k(x_i, x_j) = phi(x_i)^T*phi(x_j) , здесь T - транспонирование

# Одна из наиболее широко используемых ядерных функций представлена ядром из функции диального базиса (ядром RBF), или гауссовым ядром:
# k(x_i, x_j) = exp( -gamma * || x_i - x_j||^2), здесь gamma - свободный параметр, который нужно оптимизировать.

# Грубо говоря, термин ядро можно нтерпретировать, как функцию подобия между парой образов.
# Благодаря экспонециальному члену, результирующий показатель подобия попадает в диапазон между 1 (для строго подобных одразцов)
# и 0 (для строго не подобных)

# --Натренируем, используя ядерную функцию модель таким образом, чтобы она могла провести нелинейную границу решения на данных XOR
from sklearn.svm import SVC
svm = SVC(kernel='rbf', gamma=0.10, C=10.0, random_state=0)
svm.fit(X_xor, y_xor)
from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt
plot_decision_regions(X_xor, y_xor, clf =svm)
plt.legend(loc = 'upper left')
plt.show()
# Параметр gamma, который мы установили равным 0.1 можно понимать, как параметр отсечения гауссовой сферы.
# Если увеличить gamma, то увеличивается влияние или охват тренировочных образцов, что ведет к более мягкой границе решения.

# --Чтобы получить более полное понимание параметра gamma, применим SVM с ядром из функции радиального базиса (RBF) к набору
# данных цветков ириса

# Подготовка данных
# Импорт основных библиотек
from sklearn import datasets
import numpy as np

# прогружаем стандартную библиотеку
iris = datasets.load_iris()

# длина и ширина лепестков цветка ириса
X = iris.data[:,[2,3]]
# метки классов, которые присутствуют
y = iris.target
# все закодировано в числовом формате для производительности
print(np.unique(y))

# оценка модели на ранее не встречавшихся данных
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# стандартизация признаков из модуля preprocessing
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
# вычисление параметров распределения для train данных (стандартное отклонение и мат. ожидание)
# для каждого набора признаков. После вызова trasform мы стандартизируем тестовые и тренинговые данные.
# Для стандартизации тестового набора мы используем теже самые параметры, вычисленные для train набора.
# Поэтому значения в тренировочном и тестовом наборе сопоставимы.
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

# -- Непосредственно анализ данных

from sklearn.svm import SVC
svm = SVC(kernel='rbf', C=1.0, random_state=0, gamma=0.2)
svm.fit(X_train_std, y_train)
from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt
plot_decision_regions(X_combined_std, y_combined, clf =svm, res=0.02)
plt.xlabel('длина лепестка [стандартизованная]')
plt.ylabel('ширина лепестка [стандартизованная]')
plt.legend(loc = 'upper left')
plt.show()
# Учитывая, что мы выбрали относитльно малую величину для параметра gamma, итоговая граница решения модели SVM с ядром из RBF будет относительно
# мягкой.

# Увеличим параметр gamma проследим за влиянием на границу решения:
from sklearn.svm import SVC
svm = SVC(kernel='rbf', C=1.0, random_state=0, gamma=100)
svm.fit(X_train_std, y_train)
from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt
plot_decision_regions(X_combined_std, y_combined, clf =svm, res=0.02)
plt.xlabel('длина лепестка [стандартизованная]')
plt.ylabel('ширина лепестка [стандартизованная]')
plt.legend(loc = 'upper left')
plt.show()
 # Теперь видно, что граница решения вокруг классов 0 и 1 гораздо компактнее
 # Ясно, что модель очень хорошо подогнана под тренировочные данные, такой классификатор будет иметь высокую ошибку обобщения.
 # Т.е. очен важно правильно подобрать параметром gamma для управления переобучением.
