# Core verctor machine: Fast SVM Training on Very Large Data Sets
# Ядерные машины опорных векторов.

# Классы библиотек Scikit-Learn Perceptron и LogisticRegression, которые были использованы ранее, используют
# высокооптимизированную библиотеку Liblinear на C/C++, аналогично класс SVC для тренировки модели SVM использует
# библиотеку Livsvm, специализированную под методы опорных векторов.

 # Преимущество этих библиотек в том, что  они позволяют достаточно быстро тренировать большое число линейных классификаторов.
 # Иногда наборы данных слишком большие для того, чтобы уместить их в памяти компьютеров. По этим причинам в библиотеке
 # Scikit-Learn существует реализация средствами класса SGDClassifier. Причем этот класс, помимо прочего, поддерживает и
 # динамическое (онлайн) обучение, используя для этого метод partial_fit.

# В основе класса SGDClassifier лежит идея, которая похожа на алгоритм стохастического градиента.
# Инициализировать логистическую регрессию и метод опорных векторов в версии со стохастическим градиентным спуском с параметрами
# по умолчанию можно следующим образом:
#
# from sklearn.linear_model import SGDClassifier
# ppn = SGDClassifier(loss='Perceptron')
# lr = SGDClassifier(loss='log')
# svm = SGDClassifier(loss='hinge')

# Еще одна причина популярности SVM состоит в том, что эти модели легко кернелизировать, т.е. модифицировать с использованием ядра
# для решения нелинейных задач классификации.

# Сгенерируем линейно неразделимый набор данных, который поможет нам (вид логического элемента XOR, используя Numpy) с примером данного
# типа классификации. Используя функцию logical_xor сгенерируем набор данных, где первым 100 элементам назначается класс 1,
# последним 100 элементам назначим класс -1.

# Сделаем так, чтобы у нас появились данные XOR со случайным шумом
# Очевидно, что такие данные не могут быть разделимы линеной гиперплоскостью, используя линеной логистической регресии и
# линейной модели SVM
import numpy as np
np.random.seed(0)
X_xor = np.random.randn(200,2)
y_xor = np.logical_xor(X_xor[:,0] > 0, X_xor[:,1] > 0)
y_xor = np.where(y_xor,1, -1)

import matplotlib.pyplot as plt
plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1 , 1] ,
            c='b', marker='x', label='1')
plt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1],
            c='r', marker='s', label='-1')
plt.ylim(-3.0)
plt.legend()
plt.show()

# Ключевая идея в основе ядерных методов для решения задач с такими линейно неразделимыми данными состоит в том, чтобы создать
# нелинейные комбинации исходных признаков и функцией отображения phi спроецировать их на пространство более высокой размерности,
# где они становятся линейно разделимыми.
