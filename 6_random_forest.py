# Подготовка данных
# Импорт основных библиотек
from sklearn import datasets
import numpy as np

# прогружаем стандартную библиотеку
iris = datasets.load_iris()

# длина и ширина лепестков цветка ириса
X = iris.data[:,[2,3]]
# метки классов, которые присутствуют
y = iris.target
# все закодировано в числовом формате для производительности
print(np.unique(y))

# оценка модели на ранее не встречавшихся данных
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# стандартизация признаков из модуля preprocessing
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
# вычисление параметров распределения для train данных (стандартное отклонение и мат. ожидание)
# для каждого набора признаков. После вызова trasform мы стандартизируем тестовые и тренинговые данные.
# Для стандартизации тестового набора мы используем теже самые параметры, вычисленные для train набора.
# Поэтому значения в тренировочном и тестовом наборе сопоставимы.
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

# --Классификация с помощью метода Random Forest
# n_jobs - параметр, который отвечает за количество задействованных ядер процессора, т.к. алгоритм отлично параллелится
# n_estimators - количество используемых для построения деревьев решений

# Описание алгоритма:

# 1. Из тренировочного набора извлекается случайная бутстреп-выборка размера n (с возвратом)
# 2. Из полученной выборки выращивается дерево решений, при этом, в каждом узле:
#     2.1 В каждом узле выбирается d признаков без возврата
#     2.2 Расщепляется узел, используя критерий наилучшего расщепления, согласно выбранной целевой функции (энтропия в нашем примере ниже)
#     2.3 Повторить шаги 1 и 2 k число раз
#     2.4 Для назначения метки класса агрегировать прогнозы из каждого дерева выбрать максимальный

# Нам не нужно подрезать лес решений, т.к. ансамблевая модель довольно устойчива к шуму одного конкретного дерева решений
# Благодаря выбору параметра n мы управляем компромиссом между смещением и дисперсией итоговой модели, увеличивая n мы убираем
# "случайность" и, скорее всего, лес будет сильно переобучен.
#
# Число n обычно выбирается таким образом, чтобы оно было эквивалентно числу образцов в исходном тренировочном наборе.
# Число d = sqrt(m) - число признаков в каждом расщеплении, где m - число признаков в исходном тренировочном наборе.


from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion="entropy", n_estimators=10, random_state=1, n_jobs=2)

forest.fit(X_train_std, y_train)

from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt
plot_decision_regions(X_combined_std, y_combined, clf = forest, res=0.02)
plt.xlabel('длина лепестка [стандартизованная]')
plt.ylabel('ширина лепестка [стандартизованная]')
plt.legend(loc = 'upper left')
plt.show()
