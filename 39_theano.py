import theano
from theano import tensor as T

# Инициализация
x1=T.scalar()
w1=T.scalar()
w0=T.scalar()
z1=w1*x1+w0

# Компиляция
net_input=theano.function(inputs=[w1, x1, w0], outputs=z1)

# Исполнить
print('Чистый вход: %.2f' % net_input(2.0, 1.0, 0.5))

# Настройка для вычисления не на процессоре, а на gpu
print(theano.config.floatX)
theano.config.floatX = 'float32'
print(theano.config.floatX)

# Bash - команда
# export THEANO_FLAGS=floatX=float32

# Применить формат только к конкретному сценарию
# THEANO_ FLAGS=floatX=float32 python your_script.py

# Настройка переключения между CPU и GPU
print(theano.config.device)

# Код на cpu из Bash
# THEANO_FLAGS=device=cpu,floatX=float64 python your_script.py

# Код на gpu из Bash
# THEANO_FLAGS=device=gpu,floatX=float32 python your_script.py

# Для постоянного использования на gpu и 32-битную систему, в домашнем каталоге создать файл .theanorc и записать туда:
echo -e "\n[global]\nfloatX=float32\ndevice=gpu\n" >> ~/.theanorc

# Если не MacOS или Linux, то можно вручную создать файл
[global]
floatX=float32
device=gpu

# -=-=-=-=-=-=-=- Работа с матричными структурами
import numpy as np

# Инициализировать
# Если Theano используется в 64-разрядном режиме, то вам нужно использовать dmatrix вместо fmatrix
# При создании тензорной переменной fmatrix, мы использовали дополнительный именованный аргумент (х)
# Если распечатать символ х тензорной переменной fmatix, не давая ему имени name, функция печати print возвратит его тип тензора TensorType,
# однако, если бы функция была инициализирована с именованным аргументом х, то print вернет именно его, а доступ к типу тензора можно
# получить, используя метод type
x=T.fmatrix(name='x')
x_sum=T.sum(x, axis=0)
print(x)
print(x.type())


# скомпилировать
calc_sum=theano.function(inputs=[x], outputs=x_sum)

# выполнить(сначала Python)
ary=[[1,2,3],[1,2,3]]
print('Сумма столбца:', calc_sum(ary))


# выполнить(массив NumPy)
ary=np.array([[1,2,3],[1,2,3]], dtype=theano.config.floatX)
print('Сумма столбца:', calc_sum(ary))

# Работая с библиотекой Theano, нам нужно выполнить всего три основных шага: определить переменную, скомпилировать код и выполнить его.
# Предыдущий пример показывает, что Theano может работать, как с типами Python, так и с типами NumPy

# Theano также имеет умную систему управления памятью, которая для ускорения работы использует память повторно.
# Если говорить более конкретно, то Theano распределяет память по нескольким устройствам, ЦП и ГП, чтобы отслеживать изменения в объеме
# памят, она назначет псевдонимы соотвествующим буферам. Далее рассмотрим переменную shared, предоставляющая возможность распределять
# большие объекты (массивы) и предоставлять нескольким функциям чтения и записи право доступа, в результате чего после компиляции мы
# также имеем возможность выполнять обновление на этих объектах.

# Инициализация
x=T.fmatrix('x')
w=theano.shared(np.array([[0.0, 0.0, 0.0]], dtype=theano.config.floatX))

z=x.dot(w.T)
# Мы определили переменную update, где мы объявили, что хотим обновлять массив w значением 1.0 после каждой итерации цикла for
update=[[w,w+1.0]]

# скомпилировать
net_input=theano.function(inputs=[x], updates=update,outputs=z)

# Исполнить
data=np.array([[1,2,3]], dtype=theano.config.floatX)

for i in range(5):
    print('z%d:' %i, net_input(data))

# Еще один ловкий трюк заключается в том, чтобы использовать переменную givens для вставки значений в граф перед его компиляцией.
# Используя этот подход, мы сможем сократить количество перемещений из оперативной памяти в через цп в гп, ускоряя работу алгоритмов
# обучения, в которых используются совместно используемые (общие) переменные. Если в компиляторе theano.function  использовать
# параметр inputs, то данные будут перемещаться из ЦП в ГП многократно, например, если мы многократно (эпохи) будем выполнять
# итерации по набору данных во время градиентного спуска.
# Используя givens, мы можем держать набор данных на ГП, если он помещается в его памяти.

# Инициализация
data=np.array([[1,2,3]], dtype=theano.config.floatX)
x=T.fmatrix('x')
w=theano.shared(np.asarray([[0.0, 0.0, 0.0]], dtype=theano.config.floatX))
z=x.dot(w.T)
update=[[w, w+1.0]]

# Скомпилировать
net_input=theano.function(inputs=[],updates=update, givens={x: data}, outputs=z)

# Исполнить
for i in range(5):
    print('z:', net_input())
# Отрибут givens - это словарь Python, который ставит в соответствие имени переменной фактический обзор Python. Мы назначили это имя, когда
# определяли fmatrix

# -=-=-=-=-=-=-=-=- Линейная регрессия
# Построим регрессию по методу наименьших квадратов (МНК, OLS).
# Создадим небольшой одномерный массив с 10 тренировочными образцами.
X_train=np.asarray([[0.0], [1.0], [2.0], [3.0], [4.0], [5.0], [6.0], [7.0], [8.0], [9.0]], dtype=theano.config.floatX)
y_train=np.asarray([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0], dtype=theano.config.floatX)
# Когда мы конструируем массивы NumPy, мы используем theano.config.floatX, с тем, чтобы произвольно переключаться туда и обратно между ЦП и ГП.

# Дадее реализуем тренировочную функцию для извлечения весов линейной регрессионной модели с использованием функции стоимости в виде
# суммы квадратичных ошибок. Отметим, что w0-это узел смещения.
import theano
from theano import tensor as T
import numpy as np

def train_linreg(X_train, y_train, eta, epochs):

    costs = []

    # Инициализировать массивы
    eta0=T.fscalar('eta0')
    y=T.fvector(name='y')
    X=T.matrix(name='X')
    w=theano.shared(np.zeros(shape=(X_train.shape[1]+1),dtype=theano.config.floatX), name='w')

    # Вычислить стоимость
    net_input=T.dot(X, w[1:]) + w[0]
    errors=y-net_input
    cost=T.sum(T.pow(errors,2))

    # Выполнить корректировку градиента
    # Функция grad автоматически вычисляет производную выражения относительно его параметров
    gradient=T.grad(cost,wrt=w)
    update=[(w, w-eta0 * gradient)]

    # Скомпилировать модель
    train=theano.function(inputs=[eta0], outputs=cost, updates=update, givens={X: X_train, y: y_train,})

    for _ in range(epochs):
        costs.append(train(eta))

    return costs, w

# После того, как мы реализовали тренировочную функцию, натренируем нашу линейную регрессионную модель и посмотрим на значения функции
# стоимости в виде суммы квадратичных ошибок (SSE), чтобы проверить ее сходимость:
import matplotlib.pyplot as plt
costs, w = train_linreg(X_train, y_train, eta=0.001, epochs=10)
plt.plot(range(1, len(costs)+1), costs)
plt.tight_layout()
plt.xlabel('Эпохи')
plt.ylabel('Стоимость')
plt.show()
# Как видно, алгоритм сходится уже после 5-ой эпохи

# Функция для предсказаний на основе входных признаков
def predict_linreg(X, w):
    Xt=T.matrix(name='X')
    net_input=T.dot(Xt, w[1:]) + w[0]
    predict=theano.function(inputs=[Xt], givens={w: w}, outputs=net_input)

    return predict(X)

# Построим график подгонки линейной регрессии на тренировочных данных
plt.scatter(X_train, y_train, marker='s', s=50)
plt.plot(range(X_train.shape[0]), predict_linreg(X_train, w), color='gray', marker='o', markersize=4, linewidth=3)
plt.xlabel('x')
plt.ylabel('y')
plt.show()

# Существуют различные функции активации, как линейные, так и не линейные. Но сумма линейных функций в конечном счете дает линейную функцию,
# именно поэтому для скрытых и выходных слоев используются нелинейные функции, чтобы внести в структуру типичной нейронной сети нелинейность,
# чтобы быть в состоянии решать сложные практические задачи.
# Логистическая функция активации наиболее близко имитирует концепцию нейрона в головном мозгу человека, ее можно представить, как вероятность
# того, сработает нейрон или нет. Однако, логистические функции активации могут быть проблематичными, если у нас есть отрицательные выходы,
# поскольку в этом случае выход из сигмоидальной функции будет близким к нулю. Если сигмоидальная функция дает на выходе значения, которые
# близки к нулю, то нейронная сеть будет тренироваться очень медленно и повышается вероятность того, что во время тренировке она застрянет
# в локальном минимуме. Имеено по этой причине в скрытых слоях предпочитают использовать функцию гиперболического тангенса.

# Логистическая функция активации не дает вероятность принадлежности классу в задаче многоклассовой классификации (сумма может быть больше 100%).
# Именно поэтому есть функция softmax - обобщение логистической функции, которая позволяет вычислять содержательные вероятности классов.
