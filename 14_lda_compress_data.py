# linead disriminant analysis (LDA)
# Сжатие данных с учителем путем линейного дискриминантного анализа

# Линейный дискриминантный анализ, он же канонический, может использоваться в качестве метода для выделения признаков в целях увеличения
# эффективности и уменьшения степени переподгонки из-за проблемы проклятия размерности в нерегуляризованных моделях.

# Основная задача LDA - найти подпространство признаков, которое оптимизирует разделимость классов.
# И метод PCF и LDA являются методами линейного преобразования, которые могут использоваться для снижения числа размерностей
# в наборе данных, но PCA - алгоритм без учителя, LDA - алгоритм с учителем.

# Одно из допущений в LDA состоит в том, что данные нормально распределены. Кроме того, мы также допускаем, что классы имеют идентичные
# ковариационные матрицы и что признаки статистически независимы друг от друга. Однако, если даже эти допущения нарушаются, метод
# LDA может все еще работать достаточно хорошо.

# Основные шаги алгоритма:

# 1. Стандартизировать d-мерный набор данных
# 2. Для каждого из классов вычислить d-мерный вектор средних
# 3. Создать матрицу разброса между классами S_b и матрицу разброса внутри классов S_w
# 4. Вычислить собственные векторы и соответствующие собственные значения матрицы {(S_w)^-1} * S_b
# 5. Выбрать k собственных векторов, которые соответствуют k самым большим собственным значениям для построения d x k - матрицы преобразования W;
#    собственные векторы являются столбцами этой матрицы.
# 6. Спроецировать образцы на новое подпространство признаков при помощи матрицы преобразования W
