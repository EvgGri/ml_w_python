# -=-=-=-=-=-=-=- Улучшение простой сети в Keras посредством добавления скрытых слоев
# Первое улучшение – включить в сеть дополнительные слои.
# После входного слоя поместим первый плотный слой с N_HIDDEN нейронами и функцией активации relu.
# Этот слой называется скрытым, потому что он напрямую не соединен ни с входом, ни с выходом.
# После первого скрытого слоя добавим еще один, также содержащий N_HIDDEN нейронов, а уже за ним будет расположен выходной слой с 10 нейронами,
# которые возбуждаются, если распознана соответствующая цифра.

# Второе улучшение совсем простое. Мы применим прореживание – с вероятностью dropout будем случайным образом отбрасывать некоторые значения,
# распространяющиеся внутри сети, состоящей из плотных скрытых слоев. Это хорошо известная форма регуляризации в машинном обучении.
# Как ни странно, отбрасывание некоторых значений приводит к повышению качества.

from __future__ import print_function
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import SGD
from keras.utils import np_utils

# Для воспроизводимости результатов
np.random.seed(1671)

# Сеть и ее обучение
NB_EPOCH = 250
BATCH_SIZE = 128
VERBOSE = 1
NB_CLASSES = 10 # количество результатов = числу цифр
OPTIMIZER = SGD() # СГС-оптимизатор, обсуждается ниже в этой главе N_HIDDEN = 128
N_HIDDEN = 128
VALIDATION_SPLIT=0.2 # какая часть обучающего набора зарезервирована для контроля
DROPOUT = 0.3

# Данные: случайно перетасованы и разбиты на обучающий и тестовый набор
(X_train, y_train), (X_test, y_test) = mnist.load_data()
# X_train содержит 60000 изображений размера 28x28 --> преобразуем в массив 60000 x 784
# Во входном слое с каждым пикселем изображения ассоциирован один нейрон, т. е. всего получается 28 × 28 = 784 нейрона
RESHAPED = 784

X_train = X_train.reshape(60000, RESHAPED)
X_test = X_test.reshape(10000, RESHAPED)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# Нормирование
# Обычно значения, ассоциированные с пикселями, нормируются с целью привести их к диапазону [0, 1]
# Это значит, что яркость каждого пикселя делится на максимально возможную яркость 255
X_train /= 255
X_test /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

# Преобразование векторов классов в бинарные матрицы классов
# На выходе получается 10 классов, по одному для каждой цифры
Y_train = np_utils.to_categorical(y_train, NB_CLASSES)
Y_test = np_utils.to_categorical(y_test, NB_CLASSES)

# Последний слой состоит из единственного нейрона с функцией активации softmax, являющейся обобщением сигмоиды.
# Softmax сплющивает k­мерный вектор, содержащий произвольные вещественные числа, в k­мерный вектор вещественных чисел из интервала (0, 1).
# В нашем случае она агрегирует 10 ответов, выданных предыдущим слоем из 10 нейронов

# M_HIDDEN скрытых слоев
# 10 выходов
# на последнем этапе softmax
model = Sequential()
model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))
model.add(Activation('relu'))
model.add(Dropout(DROPOUT))
model.add(Dense(N_HIDDEN))
model.add(Activation('relu'))
model.add(Dropout(DROPOUT))
model.add(Dense(NB_CLASSES))
model.add(Activation('softmax'))
model.summary()

# Just disables the warning, doesn't enable AVX/FMA // игнорирование ошибки
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Определенную таким образом модель необходимо откомпилировать, т. е. привести к виду, допускающему исполнение базовой библиотекой (Theano или TensorFlow).

# Перед компиляцией необходимо принять несколько решений:

# 1. выбрать оптимизатор, т. е. конкретный алгоритм, который будет обновлять веса в процессе обучения модели
# 2. выбрать целевую функцию, которую оптимизатор использует для навигации по пространству весов (часто целевая функция называется также функцией потерь)
# 3. оценить качество обученной модели

# Перечень целевых функций, поддерживаемых Keras, приведен на странице https://keras.io/objectives/
# 1. Среднеквадратическая ошибка (СКО): Если предсказание сильно отличается от истинного значения, то возведение в квадрат делает отличие еще более явственным.
# 2. Бинарная перекрестная энтропия: Эта целевая функция подходит для предсказания бинарных меток
# 3. Категориальная перекрестная энтропия: Эта целевая функция подходит для предсказания много- классовых меток. Она же по умолчанию используется совместно
# с функцией активации softmax

# Ниже перечислено несколько популярных показателей качества (полный список см. на странице https://keras.io/metrics/):
# 1. верность: отношение числа правильных предсказаний к общему числу меток
# 2. точность: доля правильных ответов модели
# 3. полнота: доля обнаруженных истинных событий

# Показатели качества похожи на целевые функции, различаются они только тем, что показатели используются не для обучения модели, а для оценки ее качества.
# Компиляция модели в Keras производится просто:
model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])

# Для обучения откомпилированной модели служит функция  fit(), принимающая в частности следующие параметры:
# 1.  epochs: число периодов – сколько раз обучающий набор предъявляется модели.
#     На каждой итерации оптимизатор пытается подкорректировать веса, стремясь минимизировать целевую функцию
# 2.  batch_size: сколько обучающих примеров должен увидеть оптимизатор, прежде чем он обновит веса
history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)

# После того как модель обучена, ее следует проверить на тестовом наборе, который содержит ранее не предъявлявшиеся примеры.
# Таким образом, мы сможем получить минимальное значение, достигаемое целевой функцией, и наилучшее значение показателя качества.
score = model.evaluate(X_test, Y_test, verbose=VERBOSE)
print("Test score:", score[0])
print('Test accuracy:', score[1])
