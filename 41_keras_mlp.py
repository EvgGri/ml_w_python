# Перцептрон – это простой алгоритм, который получает входной вектор x, содержащий n значений (x1, x2, ..., xn), которые часто
# называются входными признаками, или просто признаками, и выдает на выходе 1 (да) или 0 (нет).


# Здесь w – вектор весов, wx – скалярное произведение, а b – смещение.

# Исходным строительным блоком Keras является модель, а простейшая модель называется последовательной.
# В Keras последовательная модель представляет собой линейный конвейер (стек) слоев нейронной сети.
# В следующем фрагменте определен один слой с 12 нейронами, который ожидает получить 8 входных переменных (признаков).

from tensorflow import keras
from keras.models import Sequential
from keras.layers.core import Dense

model = Sequential()

# На этапе инициализации каждому нейрону можно назначить вес. Keras предлагает несколько вариантов:
# random_uniform: веса инициализируются равномерно распределенными случайными значениями из диапазона (–0.05, 0.05)
# random_normal: веса инициализируются нормально рас- пределенными случайными значениями со средним 0 и стандартным отклонением 0.05
# zero: все веса инициализируются нулями
model.add(Dense(12, input_dim=8, kernel_initializer='random_uniform'))

# -=-=-=-=-=-=-=-=- Распознавание цифр

# Воспользуемся библиотекой Keras, чтобы определить сеть, распознающую рукописные цифры из набора MNIST.
# Начнем с очень простой нейросети и постепенно будем ее улучшать.
from __future__ import print_function
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.optimizers import SGD
from keras.utils import np_utils
np.random.seed(1671) # для воспроизводимости результатов

# Сеть и ее обучение
NB_EPOCH = 200
BATCH_SIZE = 128
VERBOSE = 1
NB_CLASSES = 10 # количество результатов = числу цифр
OPTIMIZER = SGD() # СГС-оптимизатор, обсуждается ниже в этой главе N_HIDDEN = 128
VALIDATION_SPLIT=0.2 # какая часть обучающего набора зарезервирована для контроля

# Данные: случайно перетасованы и разбиты на обучающий и тестовый набор
(X_train, y_train), (X_test, y_test) = mnist.load_data()
# X_train содержит 60000 изображений размера 28x28 --> преобразуем в массив 60000 x 784
# Во входном слое с каждым пикселем изображения ассоциирован один нейрон, т. е. всего получается 28 × 28 = 784 нейрона
RESHAPED = 784

X_train = X_train.reshape(60000, RESHAPED)
X_test = X_test.reshape(10000, RESHAPED)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# Нормирование
# Обычно значения, ассоциированные с пикселями, нормируются с целью привести их к диапазону [0, 1]
# Это значит, что яркость каждого пикселя делится на максимально возможную яркость 255
X_train /= 255
X_test /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

# Преобразование векторов классов в бинарные матрицы классов
# На выходе получается 10 классов, по одному для каждой цифры
Y_train = np_utils.to_categorical(y_train, NB_CLASSES)
Y_test = np_utils.to_categorical(y_test, NB_CLASSES)

# Последний слой состоит из единственного нейрона с функцией активации softmax, являющейся обобщением сигмоиды.
# Softmax сплющивает k­мерный вектор, содержащий произвольные вещественные числа, в k­мерный вектор вещественных чисел из интервала (0, 1).
# В нашем случае она агрегирует 10 ответов, выданных предыдущим слоем из 10 нейронов

# 10 выходов
# На последнем этапе softmax
model = Sequential()
model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))
model.add(Activation('softmax'))
model.summary()
