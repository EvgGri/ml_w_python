# Набор данных, содержащий информацию о зданиях в пригороде Бостона.
import pandas as pd
url='./data/BostonHousing.xls'

df=pd.read_excel(url, index_col=None)
df.columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']
df.head()

 # -=-=-=-=-=-=-=-=-=-= Exploratory data analysis, EDA -  Разведочный анализ
 # Создадим матрицу точечных графиков:
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='whitegrid', context='notebook')
cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV']
sns.pairplot(df[cols], size=2.5)
plt.show()
# Вернуть стандартные стили
# sns.reset_orig()

# Корреляционная матрица - это квадратная матрица, которая содержит линеные коэффицLinearиенты корреляции Пирсона, которые измеряют линейную
# зависимость между парами признаков. Коэффициенты корреляции ограничены диапазоном [-1,1].
# Вопреки мнению, что во время тренировки линейной регрессионной модели необходимо, чтобы объясняющие, либо целевые переменные были
# распределены нормально, допущение о нормальности распределения является необходимым условияем для определенных статистических тестов.
import numpy as np
cm = np.corrcoef(df[cols].values.T)
sns.set(font_scale=1.5)
hm=sns.heatmap(cm,cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':15}, yticklabels=cols, xticklabels=cols)
plt.show()
# Для того, чтобы выполнить подгонку линейной регрессионной модели, нас интересуют те признаки, которые имеют высокую корреляцию с нашей целевой
# переменной MEDV

# Реализация обычной регрессионной модели методом наименьших квадратов (OLS - Ordinary least squares)
from sklearn.linear_model import LinearRegression
slr = LinearRegression()

X=df[['RM']].values
y=df[['MEDV']].values

slr.fit(X,y)
print('Наклон: %.3f' % slr.coef_[0])
print('Пересечение: %.3f' % slr.intercept_)

def lin_regplot(X,y, model):
    plt.scatter(X,y,c='blue')
    plt.plot(X, model.predict(X), color='red')
    return None

lin_regplot(X,y,slr)

plt.xlabel('Среднее число комнат [RM]')
plt.ylabel('Цена в тыс. долл. [MEDV]')
plt.show()

# -=-=-=-=-=-=-=-=-=-= Подгонка стабильной регрессионной модели алгоритмом RANSAC
# Выбросы могут оказывать сильное воздействие на линейные модели. Небольшое подмножество в данных может оказать очень сильное влияние
# на результаты моделирования. Как альтернатива статистическим методам, позволяющим исключить выбросы, предлагается испольовать
# устойчивый метод регрессии с использованием алгоритма RANSAC (Random Sample Consensus, т.е. консенсус на подмножестве случайных образцов).
# Он выполняет подгонку регрессионной модели на подмножестве данных, так называемых не-выбросах (inliers), т.е. на хороших точках данных.

# Общая схема алгоритма:
# 1. Выбрать случайное число образцов в качестве не-выбросов и выполнить подгонку модели
# 2. Проверить все остальные точки данных на подогнанной модели и добавить те точки, которые попадают в пределы заданного аналитиком диапазона
# для не-выбросов
# 3. Выполнить повторную подгонку модели с использованием всех не-выбросов
# 4. Оценить ошибку подогнанной модели относительно не-выбросов
# 5. Завершить алгоритм, в случае если качество соответствует определенному заданному пользователю порогу либо если было достигнуто
# фиксированное число итераций

from sklearn.linear_model import RANSACRegressor
ransac = RANSACRegressor(LinearRegression(),
                         max_trials=100,
                         min_samples=50,
                         residual_metric=lambda x: np.sum(np.abs(x), axis=1),
                         residual_threshold=5.0,
                         random_state=0)
ransac.fit(X,y)
# max_trials - максиимальное число итераций
# min_samples - минимальное число случайно отобранных образцов
# residual_metric - параметр метрики остатков, лямбда функция рассчитывает абсолютные вертикальные расстояния между точками образцов и
# подогнанной линией
# residual_threshold - разрешаем включать в подмножество не-выбросов образцы с вертикальным расстоянием не больше 5 единиц

# После подгонки модели RANSAC получим не-выбросы и выбросы из подогнанной линейной регрессионой модели RANSAC и построим совместный график с
# линейной подгонкой
inlier_mask = ransac.inlier_mask_
outlier_mask = np.logical_not(inlier_mask)
line_X = np.arange(3,10,1)
line_y_ransac = ransac.predict(line_X[:, np.newaxis])
plt.scatter(X[inlier_mask], y[inlier_mask], c='blue', marker='o', label='Не-выбросы')
plt.scatter(X[outlier_mask], y[outlier_mask], c='red', marker='s', label='Выбросы')
plt.plot(line_X, line_y_ransac, color='red')
plt.xlabel('Среднее число комнат [RM]')
plt.ylabel('Средняя цена в тыс. долл. [MEDV]')
plt.legend(loc='upper left')
plt.show()
# Если мы распечатаем наклон (угловой коэффициент) и точку пересечения модели, увидим, что линия линейной регрессии отличается от подгонки,
# которую мы получили в предыдущем разделе без модели RANSAC
print('Наклон: %.3f' % ransac.estimator_.coef_[0])
print('Пересечение: %.3f' % ransac.estimator_.intercept_)

# -=-=-=-=-=-=-=-=-=-= Оценка качества работы линейных регрессионных моделей
# Вместо того, чтобы использовать простую регрессионную модель используем все переменные набора данных и натренируем множественную
# регрессионную модель.
from sklearn.model_selection import train_test_split
X = df.iloc[:,:-1].values
y = df['MEDV'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
slr = LinearRegression()
slr.fit(X_train, y_train)
y_train_pred = slr.predict(X_train)
y_test_pred = slr.predict(X_test)

# Учитывая, что наша модель использует две и более объясняющих переменных, мы не можем визуально представить линию регрессии на
# двумерном графике, но мы можем вывести на график остатки в сопоставлении с предсказанными значениями.
plt.scatter(y_train_pred, y_train_pred - y_train, c='blue', marker='o', label='Тренировочные данные')
plt.scatter(y_test_pred, y_test_pred - y_test, c='lightgreen', marker='s', label='Тестовые данные')
plt.xlabel('Предсказанные значения')
plt.ylabel('Остатки')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red')
plt.xlim([-10,50])
plt.show()
# В случае идеальной модели мы увидели бы нулевые остатки. Однако, от хорошей регрессионной модели мы ожидаем что ошибки распределены
# случайно, а остатки случайно разбросаны вокруг средней линии. Если мы видим на графике распределения остатков повторяющиеся образы,
# значит, модель неспособна захватить некоторую объяснительную информацию, что проявилось в остатках. Также таким образом можно
# отслеживать появление выбросов, которые представлены точками с большим отклонением от средней линии.

# Еще одной полезной количественной мерой оценки качества модели является так называемая средневзвешенная квадратичная ошибка (MSE, Mean
# Square Error), т.е. просто усредненное значение функции стоимости SSE, которую мы минимизируем для подгонки линейной
# регрессионной модели.
# MSE полезна при подгонке разных регрессионных моделей или для тонкой настройки их параметров путем поиска по сетке параметров и перекрестной
# проверке.
from sklearn.metrics import mean_squared_error
print('MSE тренировка: % .3f, тестирование: %.3f' % (mean_squared_error(y_train_pred, y_train), mean_squared_error(y_test_pred, y_test)))
# Мы увидили, что MSE на тренировочном наборе сильно меньше, чем на тестовом, что означает, что наша модель сильно переподогнана под
# тренировочные данные.
# Иногда имеет смысл говорить о коэффициенте детерминации R-квадрате, который может пониматься, как стандартизованныая версия MSE в целях
# получения лучшей интерпретируемости качества модели.
# Другими словами R-квадрат это доля дисперсии отклика, которая охвачена моделью. R^2=1-SSE/SST, где SSE - это сумма квадратичных ошибок,
# SST - полная сумма квадратов.
# Для тренировочного набора данных R^2 ограничен диапазоном от 0 до 1, но он может стать отрицательным для тестового набора.
# Если R-квадрат равен 1, то модель идеально аппроксимирует данные с соответствующим  MSE = 0.

from sklearn.metrics import r2_score
print('R^2 тренировка: %.3f, тестирование: %.3f' % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))
# -=-=-=-=-=-=-=-=-=-= Применение регуляризованных методов для регрессии:
# Регуляризация - один из подходов к решению проблемы переобучения путем добавления добавления дополнительной и тем самым сжатия значений
# параметров модели, чтобы вызвать штраф за сложность.
# Самыми популярными подходами к регуляризованной линейной регресси являются метод гребневой регрессии (ridge regression) и метод lasso
# (оператор наименьшего абсолютного стягивания и отбора, least absolute shrinkage and selection operator) и метод эластичной сети elastic net

# Гребневая регрессия - это модель с L2-штрафом, где к нашей функции стоимости мы добавляем квадратичную сумму весов
# Алтернативный подход - метод lasso, в котором в зависимости от силы регуляризации определенные веса могут стать нулевыми
# Ограничение метода лассо состоит в том, что он отбирает не более "n" переменных, если m>n /* тут требуется пояснение
# Компромиссом между гребневой регрессией и методом lasso является эластичная сеть, при которой имеются L1-штраф для генерирования
# разреженности и L2-штраф для преодоления некоторых ограничений метода lasso, таких как число отобранных переменных.

# Модель гребневой регрессии можно инициализировать следующим образом:
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1.0)

# Отметим, что сила регуляризации регулируется параметром alpha, который аналогичен параметру lambda

# Лассо - регрессор инициализируется следующим образом
from sklearn.linear_model import Lasso
lasso = Lasso(alpha=1.0)

# Эластичная сеть позволяет варьировать соотношение L1 к L2, например, если установить l1_ratio равным 1.0, то регрессор эластичной сети
# будет равен lasso
from sklearn.linear_model import ElasticNet
lasso = ElasticNet(alpha=1.0, l1_ratio=0.5)

# Полиномиальная регрессия - или превращение линейной регрессионной модели в криволинейную
# В предыдущем разделе мы допустили наличие между объясняющей переменной и переменной отклика линейной связи.
# Один из способов объяснить нарушение допущения о линейной связи состоит в том, чтобы использовать модель полиномиальной регрессии.
# Несмотря на то, что полиномиальную регрессию мы будем использовать для моделирования нелинейных связей, она по прежнему рассматривается
# как модель множественной линейной регрессии, ввиду линейных коэффициентов регрессии w /* где: y = w_0 + w_1*x + ... + w_d*x^d

# Добавим в задачу простой регрессии с одной объясняющей переменной квадратичного члена (d=2) и сравнения полинома с линейной подгонкой
# Добавления члена с полиномом второй степени:
from sklearn.preprocessing import PolynomialFeatures

X = np.array([258.0, 270.0, 294.0,
              320.0, 342.0, 368.0,
              396.0, 446.0, 480.0,
              586.0]) [:, np.newaxis]

y = np.array([236.4, 234.4, 252.8,
              298.6, 314.2, 342.2,
              360.8, 368.0, 391.2,
              390.8])

lr = LinearRegression()
pr = LinearRegression()
quadratic = PolynomialFeatures(degree=2)

X_quad = quadratic.fit_transform(X)

# Подгонка простой линейной регрессии
lr.fit(X, y)
X_fit = np.arange(250,600,10) [:, np.newaxis]
y_lin_fit = lr.predict(X_fit)

# Множественная регрессионная модель на преобразованных признаках для полиномиальной регресии
pr.fit(X_quad, y)
y_quad_fit = pr.predict(quadratic.fit_transform(X_fit))

# График результатов
plt.scatter(X, y, label='тренировочные точки')
plt.plot (X_fit , y_lin_fit, label='линейная подгонка', linestyle='--')
plt.plot (X_fit , y_quad_fit, label='квадратичная подгонка')
plt.legend(loc='upper left')
plt.show()
y_lin_pred = lr.predict(X)
y_quad_pred = pr.predict(X_quad)
print('\nТренировочная MSE линейная: %.3f, квадратичная %.3f' %
      (mean_squared_error(y, y_lin_pred), mean_squared_error(y, y_quad_pred))
      )
print('\nТренировочная R^2 линейная: %.3f, квадратичная %.3f' %
      (r2_score(y, y_lin_pred), r2_score(y, y_quad_pred))
      )
