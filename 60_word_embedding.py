#-=-=-=-=-=-=-=-=- Погружения слов

# В википедии погружение, или векторное представление слов (word embedding) определяется как общее название различных методов языкового
# моделирования и обучения признаков, применяемых в обработке естественных языков (ОЕЯ, англ. NLP), когда слова или фразы из словаря
# отображаются на векторы вещественных чисел.
#
# Погружение слов – это способ преобразовать текстовое представление слов в числовые векторы, допускающие анализ стандартными алгоритмами
# машинного обучения, принимающими на входе числа.
#
# В главе 1 мы уже встречались с одним видом погружения слов – унитарным кодированием. Это самый простой подход к погружению.
# Напомним, что унитарным кодом слова будет вектор, число элементов которого равно размеру словаря, такой, что элемент, соответствующий
# данному слову, равен 1, а все остальные 0.
#
# Основная проблема унитарного кодирования в том, что нет никакого способа представить сходство слов. В любом заданном корпусе текстов
# мы ожидаем, что между словами «кошка» и «собака» или «нож» и «вилка» есть какое­то сходство. Сходство векторов вычисляется с помощью
# скалярного произведения, т. е. суммы произведений соответственных элементов.
#
# В случае унитарного кодирования скалярное произведение любых двух слов равно нулю.
# Для преодоления ограничений унитарного кодирования сообщество ОЕЯ заимствовало из информационного поиска (ИП) идею
# векторизации текста с использованием документа в качестве контекста. Здесь стоит отметить такие подходы,
# как TF­-IDF (https://en.wikipedia.org/wiki/Tf%E2%80%93idf), латентно-семантический анализ (ЛСА)
# (https://en.wikipedia.org/wiki/Latent_semantic_ analysis) и тематическое моделирование (https://en.wikipedia.org/ wiki/Topic_model).
#
# Но эти представления улавливают несколько иную, документо­центрическую, идею семантического сходства.

# Рассмотрим две формы погружения слов, GloVe и word2vec, известные под общим названием «распределенное представление слов».

# Мы также узнаем о способах порождения собственных погружений в программе на Keras, а равно о том, как использовать и настраивать
# предобученные модели на основе word2vec и GloVe.

# Будут рассмотрены следующие темы:
#  построение различных распределенных представлений слов в контексте;
#  построение моделей на основе погружений для решения таких задач ОЕЯ, как грамматический разбор предложения и анализ эмоциональной окраски.

# Распределенные представления
# Распределенное представление – это попытка уловить смысл слова путем рассмотрения его связей с другими словами в контексте.
# Эта идея сформулирована в следующем высказывании Дж. Р. Фир- та (J. R. Firth)
# (см. статью Andrew M. Dai, Christopher Olah, Quoc V. Le «Document Embedding with Paragraph Vectors», arXiv:1507.07998, 2015), лингвиста,
# который первым выдвинул ее:

# Мы узнаем слово по компании, с которой оно дружит.
# Рассмотрим такие два предложения:
# Париж – столица Франции.
# Берлин – столица Германии.

# Даже если вы совсем не знаете географию (или русский язык), все равно нетрудно сообразить, что пары слов (Париж, Берлин) и (Франция, Германия)
# как­то связаны и что между соответственными словами связи одинаковы, т. е.
# Париж : Франция :: Берлин : Германия

# Следовательно, задача распределенного представления – найти такую общую функцию φ преобразования слова в соответствующий ему вектор,
# что справедливы соотношения следующего вида:
# φ («Париж») – φ («Франция») ≈ φ («Берлин») – φ («Германия»)

# Иными словами, цель распределенного представления – преобразовать слова в векторы, так чтобы сходство векторов коррелировало с семантическим
# сходством слов.
#
# В следующих разделах мы рассмотрим два наиболее известных погружения слов: word2vec и GloVe.

#-=-=-=-=-=-=-=-=- word2vec
