# -=-=-=-=-=-=-=-=-=- Порождающие состязательные сети и WaveNet

# В этой главе мы обсудим порождающие состязательные сети (ПСС) (generative adversarial network – GAN) и сеть WaveNet.

# ПСС способны обучаться порождению синтетических данных, которые выглядят в точности как настоящие.
# Например, компьютер можно научить рисовать и создавать реалистичные изображения.
# Идея была предложена Яном Гудфеллоу (см. I. Goodfellow «NIPS 2016 Tutorial: Generative Adversarial Networks», 2016),
# который работал в Монреальском университете, в компании Google Brain и, в последнее время, в OpenAI (https:// openai.com/).
# WaveNet – глубокая порождающая сеть, предложенная компанией Google DeepMind для обучения компьютеров высококачественному
# воспроизведению человеческого голоса и звучания музыкальных инструментов.

# Что такое ПСС?
# Основная идея ПСС аналогична подделке произведений искусства, т. е. созданию работ (https://en.wikipedia.org/wiki/Art), ошибочно
# приписываемых другим, обычно более известным, авторам. ПСС обучает две нейронные сети одновременно. Генератор G(Z) порождает подделку,
# а дискриминатор D(Y) судит о том, насколько репродукция реалистична, основываясь на наблюдениях аутентичных произведений и копий.
# D(Y) принимает вход Y (например, изображение) и выражает свое суждение о его подлинности – в общем случае значение, близкое к 0,
# означает подлинный, а близкое к единице – подделка. G(Z) принимает на входе случайный шум Z и обучается обманывать D, заставляя его думать,
# что результат работы G(Z) – подлинное произведение. Таким образом, цель обучения дискриминатора – максимизировать D(Y) для всех
# изображений из истинного распределения данных и минимизировать для изображений, не выбранных из истинного распределения.
# Следовательно, G и D ведут себя как противники в некоторой игре, отсюда и название состязательное обучение. Отметим, что G и D
# обучаются попеременно, а в качестве целевой функции выступает функция потерь, оптимизируемая методом градиентного спуска.
# Порождающая модель обучается подделывать, а дискриминантная распознавать подделки. Дискриминантная сеть (обычно стандартная сверточная
# нейронная сеть) пытается классифицировать изображение как настоящее или сгенерированное.
# Важная новая идея – обратное распространение через дискриминатор и генератор с целью корректировать параметры генератора таким образом,
# чтобы генератор мог обучиться, как успешнее обманывать дискриминатор. В конечном итоге генератор научится порождать поддельные изображения,
# неотличимые от настоящих.

# Разумеется, от ПСС требуется найти точку равновесия в игре двух игроков. Чтобы обучение оказалось эффективным, необходимо, чтобы обновление,
# в результате которого один игрок опускается вниз, одновременно приводило к опусканию другого игрока. Задумайтесь об этом!
# Если фальсификатор научится обманывать арбитра в каждом случае, то самому фальсификатору больше нечему учиться.
# Иногда оба игрока достигают равновесия, но это не гарантируется, и игра может продолжаться долго.

# Теперь посмотрим, как можно научить ПСС подделывать набор данных MNIST. В этом случае в качестве генератора и дискриминатора ПСС
# используются сверточные сети (см. A. Radford, L. Metz, and S. Chintala «Unsupervised Representation Learning with Deep Convolutional
# Generative Adversarial Networks», arXiv: 1511.06434, 2015).
# В начале генератор порождает нечто неразборчивое, но после нескольких итераций синтетические цифры становятся все более отчетливыми.
# На следующем рисунке панели упорядочены по номеру итерации и, как видите, качество постепенно повышается.

# -=-=-=-=-=-=-=-=-=- Глубокие сверточные порождающие состязательные сети

# Глубокие сверточные порождающие состязательные сети (ГСПСС, англ. DCGAN) впервые описаны в статье A. Radford, L. Metz, and S. Chintala
# «Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks», arXiv: 1511.06434, 2015.
# В генераторе используется 100-­мерное пространство с равномерным распределением Z, которое проецируется на пространство меньшей
# размерности с помощью последовательности парных операций свертки.

# Генератор ГСПСС можно описать следующим кодом на Keras, имеется также другая реализация по адресу https://github.com/jacobgil/keras-dcgan
# Отметим, что в этой сверточной сети нет пулинговых операций.
def generator_model():
    model = Sequential()
    # Теперь рассмотрим код. Первый плотный слой принимает 100­-мерный входной вектор и порождает 1024 выхода, в качестве функции
    # активации используется tanh. Предполагается, что входные данные выбираются из равномерного распределения на от- резке [–1, 1].
    model.add(Dense(input_dim=100, output_dim=1024))
    model.add(Activation('tanh'))
    # Следующий плотный слой порождает на выходе тензор формы 128 × 7 × 7, применяя пакетную нормировку (см. S. Ioffe, C. Szegedy
    # «Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift», arXiv: 1502.03167, 2014) –
    # технику, помогающую стабилизировать обучение путем нормировки входных данных, так чтобы их среднее было равно нулю, а
    # дисперсия – единице. Эмпирически установлено, что пакетная нор- мировка во многих случаях ускоряет обучение, смягчает проблемы,
    # вызванные неудачной инициализацией, и вообще приводит к более точным результатам.
    model.add(Dense(128*7*7))
    model.add(BatchNormalization())
    model.add(Activation('tanh'))
    # В конвейер вставляется также модуль Reshape(), который порождает данные формы 127 × 7 × 7 (127 каналов, ширина 7, высота 7) с
    # параметром dim_ordering равным tf, и модуль повышающей передискретизации UpSampling(), который повторяет каждый пиксель
    # в квадрате 2 × 2.
    model.add(Reshape((128, 7, 7), input_shape=(128*7*7,)))
    model.add(UpSampling2D(size=(2, 2)))
    # После этого идет сверточный слой, порождающий 64 фильтра с ядром размера 5 × 5 и функцией активации tanh
    model.add(Convolution2D(64, 5, 5, border_mode='same'))
    model.add(Activation('tanh'))
    # За ним еще один модуль UpSampling() и последняя свертка с одним выходным фильтром, ядром размера 5 × 5 и функцией активации tanh.
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(1, 5, 5, border_mode='same'))
    model.add(Activation('tanh'))
    return model

# Дискриминатор описывается следующим кодом:
def discriminator_model():
    model = Sequential()
    # Мы берем изображение из стандартного набора MNIST, имеющее форму (1, 28, 28), применяем свертку с 64 фильтрами размера 5 × 5 и функцию активации tanh.
    model.add(Convolution2D(64, 5, 5, border_mode='same', input_shape=(1, 28, 28)))
    model.add(Activation('tanh'))
    # Далее следует операция max­ пулинга по области размера 2 × 2 и еще одна свертка и операция max­пулинга.
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Convolution2D(128, 5, 5))
    model.add(Activation('tanh'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    # Последние два слоя плотные, и самый верхний, дающий предсказание о подделке, состоит всего из одного нейрона с сигмоидной функцией активации.
    model.add(Flatten())
    model.add(Dense(1024))
    model.add(Activation('tanh'))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))
    return model

# На протяжении выбранного количества периодов генератор и дискриминатор по очереди обучаются с использованием binary_crossentropy в качестве функции потерь.
# В каждом периоде генератор делает ряд предсказаний (например, порождает поддельные изображения рукописных цифр), а дискриминатор пытается обучиться после смешения предсказания
# с настоящими изображениями из набора MNIST. Через 32 периода генератор обучается подделывать этот набор данных. Никто не писал программу для вывода цифр, однако машина
# научилась порождать цифры, неотличимые от написанных человеком. Отметим, что обучение ПСС может оказаться очень трудной задачей из­за необходимости поддерживать равновесие
# между обоими игроками. Если эта тема вас заинтересовала, рекомендую познакомиться с практическими приемами, описанными по адресу https://github.com/soumith/ganhacks
