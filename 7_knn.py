# Подготовка данных
# Импорт основных библиотек
from sklearn import datasets
import numpy as np

# прогружаем стандартную библиотеку
iris = datasets.load_iris()

# длина и ширина лепестков цветка ириса
X = iris.data[:,[2,3]]
# метки классов, которые присутствуют
y = iris.target
# все закодировано в числовом формате для производительности
print(np.unique(y))

# оценка модели на ранее не встречавшихся данных
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# стандартизация признаков из модуля preprocessing
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
# вычисление параметров распределения для train данных (стандартное отклонение и мат. ожидание)
# для каждого набора признаков. После вызова trasform мы стандартизируем тестовые и тренинговые данные.
# Для стандартизации тестового набора мы используем теже самые параметры, вычисленные для train набора.
# Поэтому значения в тренировочном и тестовом наборе сопоставимы.
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))


# KNN - метод K ближайших соседей (алгоритм ленивого обучения)
# K nearest neighbor classifier

# Его называют ленивым, т.к. он не извлекает дискриминантную (различающую) функцию из тренировочных данных, а
# вместо этого он просто запоминает тренировочный набор.

# Алгоритмы машинного обучения можно сгруппировать в параметрические и непараметрические модели.
# Используя параметрические модели, мы выполняем оценивание параметров из тренировочного набора данных,
# чтобы извлечь функцию, которая сможет классифицировать новые точки данных, больше не требуя исходного тренировочного набора.

# Непараметрические модели не могут быть охарактеризованы фиксированным набором параметров, и число параметров в них растет
# вместе с тренировочными данными.

# Непосредственно сам алгоритм KNN состоит из следующих шагов:
# 1. Выбрать число k и метрику расстояния
# 2. Найти k ближайших соседей образца, который мы хотим классифицировать
# 3. Присвоить метку класса мажоритарным голосованием (среди ближайших образцов)

# Основной плюс такого алгоритма в том, что классификатор мгновенно адаптируется при поступлении новых данных, при этом из минусов
# вычислительная сложность алгоритма, она растёт линейно вместе с числом образцов в тренировочном наборе (если алгоритм не был реализован
# с использованием эффективных структур данных, таких, как KD-деревья // логарифмически ожидаемое время).

 # --Реализация алгоритма с использованием евклидова расстояния в библиотеке scikit-learn
 # В случае равного числа голосов реализация в KNN в scikit-learn предпочитает соседей с наименьшим расстоянием до образца.

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')
knn.fit(X_train_std, y_train)

from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt
plot_decision_regions(X_combined_std, y_combined, clf = knn, res=0.02)
plt.xlabel('длина лепестка [стандартизованная]')
plt.ylabel('ширина лепестка [стандартизованная]')
plt.legend(loc = 'upper left')
plt.show()

# Правильный выбор k важен для нахождения баланса между переобучением и недообучением.
# Также очень важно убедиться, что мы правильно выбираем метрику расстояния, подходящую для признаков в наборе данных.
# Для образцов с вещественными значениями подойдет евклидова метрика, но, чтобы вклад каждый признак вносил одинаковый вклад в расстояние,
# необходимо стандартизировать данные.

# Расстояние Минковского, которое мы использовали в примере является прямым обобщением евклидова и манхэттенского расстояния (расстояния
# городских кварталов), которое представляет из себя следующее:
#
# d(x_i, x_j) = sqrt_p(sum_k(abs[x_i_k - x_j_k]^p))

# При p=2 это расстояние становится евклидовым, при p=1 манхэттеновское.
# KNN очень чувствительна к переобучению из-за явления проклятия размерности, когда пространство признаков становится всё более и более разреженным.
# Интуитивно можно понимать, что ближайшие соседи находятся слишком далеко, чтобы дать адекватную оценку.
