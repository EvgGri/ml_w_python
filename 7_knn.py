# Подготовка данных
# Импорт основных библиотек
from sklearn import datasets
import numpy as np

# прогружаем стандартную библиотеку
iris = datasets.load_iris()

# длина и ширина лепестков цветка ириса
X = iris.data[:,[2,3]]
# метки классов, которые присутствуют
y = iris.target
# все закодировано в числовом формате для производительности
print(np.unique(y))

# оценка модели на ранее не встречавшихся данных
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# стандартизация признаков из модуля preprocessing
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
# вычисление параметров распределения для train данных (стандартное отклонение и мат. ожидание)
# для каждого набора признаков. После вызова trasform мы стандартизируем тестовые и тренинговые данные.
# Для стандартизации тестового набора мы используем теже самые параметры, вычисленные для train набора.
# Поэтому значения в тренировочном и тестовом наборе сопоставимы.
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))


# KNN - метод K ближайших соседей (алгоритм ленивого обучения)
# K nearest neighbor classifier

# Его называют ленивым, т.к. он не извлекает дискриминантную (различающую) функцию из тренировочных данных, а
# вместо этого он просто запоминает тренировочный набор.

# Алгоритмы машинного обучения можно сгруппировать в параметрические и непараметрические модели.
# Используя параметрические модели, мы выполняем оценивание параметров из тренировочного набора данных,
# чтобы извлечь функцию, которая сможет классифицировать новые точки данных, больше не требуя исходного тренировочного набора.

# Непараметрические модели не могут быть охарактеризованы фиксированным набором параметров, и число параметров в них растет
# вместе с тренировочными данными.

# Непосредственно сам алгоритм KNN состоит из следующих шагов:
# 1. Выбрать число k и метрику расстояния
# 2. Найти k ближайших соседей образца, который мы хотим классифицировать
# 3. Присвоить метку класса мажоритарным голосованием (среди ближайших образцов)

# Основной плюс такого алгоритма в том, что классификатор мгновенно адаптируется при поступлении новых данных, при этом из минусов
# вычислительная сложность алгоритма, она растёт линейно вместе с числом образцов в тренировочном наборе (если алгоритм не был реализован
# с использованием эффективных структур данных, таких, как KD-деревья // логарифмически ожидаемое время).

 # --Реализация алгоритма с использованием евклидова расстояния в библиотеке scikit-learn
 # В случае равного числа голосов реализация в KNN в scikit-learn предпочитает соседей с наименьшим расстоянием до образца.

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')
knn.fit(X_train_std, y_train)

from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt
plot_decision_regions(X_combined_std, y_combined, clf = knn, res=0.02)
plt.xlabel('длина лепестка [стандартизованная]')
plt.ylabel('ширина лепестка [стандартизованная]')
plt.legend(loc = 'upper left')
plt.show()
