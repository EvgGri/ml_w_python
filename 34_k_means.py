# Кластеризация - алгоритм выделения группы подобных объектов.
# Алгоритм k-средних принадлежит к категории кластеризации на основе прототипов.
# Существует еще две категории кластеризации: иерархическая кластеризация и кластеризация на основе плотности.

# Кластеризация на основе прототипов означает, что каждый кластер представлен прототипом, который может быть либо центроидом (средним),
# подобных точек с непрерывными центрами, либо медоидом (наиболее представительной или наиболее часто встречающейся точкой) в случае
# категориальных признаков.

# Метод к-средних хорошо выполняет идентификацию кластеров сферической формы, но недостаток метода состоит в том, что нам необходимо
# указывать число кластеров к. Оптимальное число кластеров можно определить с помощью метода локтя и силуэтного графика.

from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=150, n_features=2, centers=3, cluster_std=0.5, shuffle=True, random_state=0)

import matplotlib.pyplot as plt
plt.scatter(X[:,0], X[:,1], c='blue', marker='o', s=50)

plt.grid()
plt.show()
# Сам алгоритм можно охарактеризовать следующими шагами:
# 1. Случайно выбрать из точек образцов к-центроидов кластеров
# 2. Назначить каждый образец самому ближайшему из центроидов
# 3. Переместить каждый центроид в центр образцов
# 4. Повторять шаги 2 и 3, пока назначения кластеров не перестанут меняться, либо пока не будет достигнут критерий остановки (число итераций)

# Подобие между объектами для образцов с непрерывными признаками определяется, как евклидово расстояние между образцами.
# Реально это просто задача оптимизации- итеративная минимизация внутрикластерной суммы квадратичных ошибок (инерция кластера).

from sklearn.cluster import KMeans
# k-априорное число кластеров, n_init-независимое выполнение алгоритма кластеризации с разными случайными центроидами, чтобы выбрать модель
# с минимальным SSE, max_iter-максимальное число итераций для каждого отдельного подхода, tol-параметр который управляет изменением SSE
# и объявляет о сходимости метода
km = KMeans(n_clusters=3, init='random', n_init=10, max_iter=300, tol=1e-04, random_state=0)

y_km = km.fit_predict(X)

# -=-=-=-=-=-=-=-=-=-=-=-=-

# До этого мы обсуждали классический алгоритм k-средних, в котором для размещения исходных центроидов используется случайное начальное число
# random seed, что иногда может давать плохие результаты для кластеризации. Один из методов борьбы с этим - это кратное выполнение алгоритма
# с разными начальными условиями и выбор наилучшей модели  с точки SSE. Другая стратегия заключается в том, чтобы помещать исходные центроиды
# как можно дальше друг от друга используя алгоритм k-means++

# Схема алгоритма следующая:
# 1. Инициаллизируем пустое множество М, для хранения к-центроидов
# 2. Случайным образом выбираем из входных образцов первый центроид u и назначаем его множеству М
# 3. Для каждого образца Х, который не находится в М, найти минимальное квадратичное расстояние до любого из центроидов
# 4. Чтобы отобрать следующий из центроид, использовать взвешенное вероятностное распределение
# 5. Повторить шаги 2 и 3, пока не будет выбрано к-центроидов
# 6. Продолжить работу с классическим лгоритмом к-средних

# Еще одна проблема заключается в том, что один и более кластеров могут быть пустыми. Эта проблема не существует для к-медоидов и алгоритма
# нечетких С-средних.
plt.scatter(X[y_km==0,0],
            X[y_km==0,1],
            s=50,
            c='lightgreen',
            marker='s',
            label='кластер 1')

plt.scatter(X[y_km==1,0],
            X[y_km==1,1],
            s=50,
            c='orange',
            marker='o',
            label='кластер 2')

plt.scatter(X[y_km==2,0],
            X[y_km==2,1],
            s=50,
            c='lightblue',
            marker='v',
            label='кластер 3')

plt.scatter(km.cluster_centers_[:,0],
            km.cluster_centers_[:,1],
            s=250,
            marker='*',
            c='red',
            label='центроиды')
plt.legend()
plt.grid()
plt.show()
# Жесткая кластеризация предполагает, что каждый образец в данных назначен строго одному кластеру.
# В отличие от них, алгоритмы мягкой кластеризации или нечеткая кластеризация назначают образец одному или более кластерам.

# -=-=-=-=-=-=-=-=-=- Алгорит мягких к-средних или нечетких к-средних

# Этот алгоритм называется FCM. Схема алгоритма очень похожа на схему алгоритма к-средних.
# Однако, вместо жесткого назначения кластеров каждой точке, мы назначаем им вероятности.
# Сумма вероятностей пренадлежности всем кластерам равна 1.

# Инерция кластера:
print('Искажение: %.2f' % km.inertia_)

# Метод локтя - графический метод, позволяющий определить какое количество кластеров необходимо взять
distortions = []

for i in range(1,11):
    km = KMeans(n_clusters=i,
                init='k-means++',
                max_iter=300,
                random_state=0)
    km.fit(X)
    distortions.append(km.inertia_)

plt.plot(range(1,11), distortions, marker='o')
plt.xlabel('Число кластеров')
plt.ylabel('Искажение')
plt.show()

# -=-=-=-=-=-=-=- Силуэтный анализ может использоваться в качестве графического инструмента для построения графика меры плотности
# группировки образцов в кластерах.

# Чтобы вычислить силуэтный коэффициент одиночного образца, можно применить следующие три шага:
# 1. Вычислить внутрикластерную связность, как среднее расстояние между образцом и всеми другими точками в том же кластере
# 2. Вычислить межкластерное разделение от следующего ближайшего кластера как среднее расстояние между образцом и всеми образцами в ближайшем
# кластере
# 3. Вычислить силуэт как разницу внутриклассовой связностью и межкластерным разделением, разделенное на большее из этих двух значений

# Силуэтный коэффициент ограничен диапазоном от -1 до 1. Коэффициент равен 0, если внутриклассовая связность равна межкластерному разделению.
# Мы приближаемся к идеальному силуэтному коэффициенту равному 1, если межкластероное разделение сильно больше внутриклассовой связности,
# поскольку межкластероное разделение говорит о том, насколько образец отличается от другого кластера, а коэффициент межкластерного разделения
# показывает насколько образец подобен элементам внутри своего кластера.

# Силуэтный коэффициент доступен как функция silhouette_samples из модуля metric
km = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0)
y_km = km.fit_predict(X)

import numpy as np
from matplotlib import cm
from sklearn.metrics import silhouette_samples

cluster_labels = np.unique(y_km)
n_clusters = cluster_labels.shape[0]
silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')

# silhouette_scores = numpy.mean(silhouette_samples()) - вычисляет средний силуэтный коэффициент

y_ax_lower, y_ax_upper = 0, 0
yticks = []
for i, c in enumerate(cluster_labels):
    c_silhouette_vals = silhouette_vals[y_km == c]
    c_silhouette_vals.sort()
    y_ax_upper += len(c_silhouette_vals)
    color = cm.jet(float(i)/n_clusters) # float(i) для совместимости с Python 2.7
    plt.barh(range(y_ax_lower, y_ax_upper),
             c_silhouette_vals,
             height=1.0,
             edgecolor='none',
             color=color)
    yticks.append((y_ax_lower + y_ax_upper)/2)
    y_ax_lower += len(c_silhouette_vals)

silhouette_avg = np.mean(silhouette_vals)
plt.axvline(silhouette_avg,
            color='red',
            linestyle='--')

plt.yticks(yticks, cluster_labels + 1)
plt.ylabel('Кластер')
plt.xlabel('Силуэтный коэффициент')
plt.show()
# Визуальный анализ силуэтного графика позволяет быстро определить`размеры разных кластеров и идентифицировать кластеры, которые содержат
# выбросы. На приведенном графике видно, что силуэтные коэффициенты даже близко не находятся рядом с 0, что может служить инидикатором
# хорошего объединения в кластеры.


# -=-=-=-=-=-=-=-=- Рассмотрим пример плохого распределения по кластерам

# Силуэтный коэффициент доступен как функция silhouette_samples из модуля metric
km = KMeans(n_clusters=2, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0)
y_km = km.fit_predict(X)

import numpy as np
from matplotlib import cm
from sklearn.metrics import silhouette_samples

cluster_labels = np.unique(y_km)
n_clusters = cluster_labels.shape[0]
silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')

# silhouette_scores = numpy.mean(silhouette_samples()) - вычисляет средний силуэтный коэффициент

y_ax_lower, y_ax_upper = 0, 0
yticks = []
for i, c in enumerate(cluster_labels):
    c_silhouette_vals = silhouette_vals[y_km == c]
    c_silhouette_vals.sort()
    y_ax_upper += len(c_silhouette_vals)
    color = cm.jet(float(i)/n_clusters) # float(i) для совместимости с Python 2.7
    plt.barh(range(y_ax_lower, y_ax_upper),
             c_silhouette_vals,
             height=1.0,
             edgecolor='none',
             color=color)
    yticks.append((y_ax_lower + y_ax_upper)/2)
    y_ax_lower += len(c_silhouette_vals)

silhouette_avg = np.mean(silhouette_vals)
plt.axvline(silhouette_avg,
            color='red',
            linestyle='--')

plt.yticks(yticks, cluster_labels + 1)
plt.ylabel('Кластер')
plt.xlabel('Силуэтный коэффициент')
plt.show()
# На итоговом графике видно, что кластеры имеют различную длину и ширину, что косвенно говорит о субоптимальном распределении кластеров.
