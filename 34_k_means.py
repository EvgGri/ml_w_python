# Кластеризация - алгоритм выделения группы подобных объектов.
# Алгоритм k-средних принадлежит к категории кластеризации на основе прототипов.
# Существует еще две категории кластеризации: иерархическая кластеризация и кластеризация на основе плотности.

# Кластеризация на основе прототипов означает, что каждый кластер представлен прототипом, который может быть либо центроидом (средним),
# подобных точек с непрерывными центрами, либо медоидом (наиболее представительной или наиболее часто встречающейся точкой) в случае
# категориальных признаков.

# Метод к-средних хорошо выполняет идентификацию кластеров сферической формы, но недостаток метода состоит в том, что нам необходимо
# указывать число кластеров к. Оптимальное число кластеров можно определить с помощью метода локтя и силуэтного графика.

from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=150, n_features=2, centers=3, cluster_std=0.5, shuffle=True, random_state=0)

import matplotlib.pyplot as plt
plt.scatter(X[:,0], X[:,1], c='blue', marker='o', s=50)

plt.grid()
plt.show()
# Сам алгоритм можно охарактеризовать следующими шагами:
# 1. Случайно выбрать из точек образцов к-центроидов кластеров
# 2. Назначить каждый образец самому ближайшему из центроидов
# 3. Переместить каждый центроид в центр образцов
# 4. Повторять шаги 2 и 3, пока назначения кластеров не перестанут меняться, либо пока не будет достигнут критерий остановки (число итераций)

# Подобие между объектами для образцов с непрерывными признаками определяется, как евклидово расстояние между образцами.
# Реально это просто задача оптимизации- итеративная минимизация внутрикластерной суммы квадратичных ошибок (инерция кластера).

from sklearn.cluster import KMeans
# k-априорное число кластеров, n_init-независимое выполнение алгоритма кластеризации с разными случайными центроидами, чтобы выбрать модель
# с минимальным SSE, max_iter-максимальное число итераций для каждого отдельного подхода, tol-параметр который управляет изменением SSE
# и объявляет о сходимости метода
km = KMeans(n_clusters=3, init='random', n_init=10, max_iter=300, tol=1e-04, random_state=0)

y_km = km.fit_predict(X)

-=-=-=-=-=-=-=-=-=-=-=-=-

# До этого мы обсуждали классический алгоритм k-средних, в котором для размещения исходных центроидов используется случайное начальное число
# random seed, что иногда может давать плохие результаты для кластеризации. Один из методов борьбы с этим - это кратное выполнение алгоритма
# с разными начальными условиями и выбор наилучшей модели  с точки SSE. Другая стратегия заключается в том, чтобы помещать исходные центроиды
# как можно дальше друг от друга используя алгоритм k-means++

# Схема алгоритма следующая:
1. Инициаллизируем пустое множество М, для хранения к-центроидов
2. Случайным образом выбираем из входных образцов первый центроид u и назначаем его множеству М
3. Для каждого образца Х, который не находится в М, найти минимальное квадратичное расстояние до любого из центроидов
4. Чтобы отобрать следующий из центроидов,
