# Альтернативный подход к кластеризации к кластеризации на основе прототипов: иерархическая кластеризация.
# Одно из преимуществ иерархической кластеризации состоит в том, что она позволяет строить дендограммы (древовидные визуализации бинарной
# иерархической кластеризации), которые способны помочь интерпретировать результаты путем создания содержательных таксономий.
# Другое преимущество состоит в том, что нам не нужно предварительно указывать число кластеров.

# Существует 2 осоновных подхода к иерархической кластеризации: агломеративный (объединительный) и дивизивный (разделяющий).
# В дивизионной иерархической кластеризации мы начинаем с единственного кластера, который охватывает все ваши образцы, и мы итеративно
# расщепляем на более мелкие кластеры, пока там не останется по одному образцу в каждом кластере.

# Сейчас мы сконцентрируемся на агломеративной кластеризации, в которой мы будем брать каждый образец, как отдельный кластер и объединять
# ближайшие пары кластеров, пока не останется всего один кластер.

# Агломеративная иерархическая кластеризация представлена двумя стандартными алгоритмами: методом одиночной связи (single linkage) или
# метод ближайших соседей, и методом полной связи (complete linkage) или метод дальнего соседа.

# Используя метод одиночной связи, для каждой пары кластеров мы вычисляем расстояния между самыми похожими членами и объединяем два
# кластера, для которых расстояние между самыми похожими членами наименьшее.

# Подход на основе полной связи подобен методу одиночной связи, но, вместо того, чтобы в каждой паре кластеров сравнивать самых похожих,
# для выполнения объединения мы сравниваем наиболее различающихся членов.

# -=-=-=-=-=-=-=-=-=-=-=- Метод агломеративной кластеризации на основе метода полной связи
# 1. Вычисляется матрица расстояний всех образцов
# 2. Представляется каждая точка данных, как одноэлементный кластер
# 3. Объединить два ближайших кластера, основываясь на расстоянии наиболее различающихся (дальних членов)
# 4. Обновить матрицу расстояний
# 5. Повторять шаги 2-3, пока не останется единственный кластер

# Вычисление матрицы расстояний
# Сгенерируем набор тестовых данных со случайными значениями:
# Строки - разные наблюдения, столбцы разные признаки этих наблюдений

import pandas as pd
import numpy as np
np.random.seed(123)
variables = ['X', 'Y', 'Z']
labels = ['ID_0','ID_1','ID_2','ID_3','ID_4']
X = np.random.random_sample([5,3])*10
df = pd.DataFrame(X, columns=variables, index=labels)
print(df[:10])

# -=-=-=-=-=-=- Вычисление матрицы расстояний с использованием spatial.distance библиотеки SciPy
from scipy.spatial.distance import pdist, squareform
row_dist = pd.DataFrame(squareform(pdist(df, metric='euclidean')), columns=labels, index=labels)
print(row_dist)

# -=-=-=-=-=-=- Применение алгоритма агломеративной кластеризации на основе метода полной связи, воспользовавшись функцией linkage,
# которая возвращает так называемую матрицу связи
from scipy.cluster.hierarchy import linkage
# help(linkage)
# Неправильный подход в этом случае - это использовать квардратную матрицу расстояний
# Правильный подход использовать сжатая матрица расстояний или используется матрица входных образцов

# Неправильный подход, используется квадратная матрица расстояний
row_clusters=linkage(row_dist, method='complete', metric='euclidean')
# Правильный подход, используется сжатая матрица расстояний
row_clusters=linkage(pdist(df, metric='euclidean'), method='complete')

# Правильный подход, используется матрица входных образцов
row_clusters=linkage(df.values, metric='euclidean', method='complete')

# Преобразование таблицы в DataFrame
pd.DataFrame(row_clusters, columns=['метка строки 1', 'метка строки 2', 'расстояние', 'число элементов в кластере'],
             index=['кластер %d' % (i+1) for i in range(row_clusters.shape[0])])
# Матрица связей состоит из нескольких строк, где каждая строка представляет собой одно объединение.
# Первый и второй столбцы обозначают наиболее различающихся членов в каждом кластере, третий столбец говорит о расстоянии между этими членами.
# Последний столбец возвращает число членов в каждом кластере.

# Теперь можно визуализировать результаты в виде дендограммы
from scipy.cluster.hierarchy import dendrogram
# делаем дендограмму черной (часть 1)
from scipy.cluster.hierarchy import set_link_color_palette
set_link_color_palette(['black'])
# row_dendr = dendrogram(row_clusters, labels=labels)
row_dendr = dendrogram(row_clusters, labels=labels, color_threshold=np.inf)
# делаем дендограмму черно (часть 2)


import matplotlib.pyplot as plt
plt.tight_layout()
plt.ylabel('Евклидово расстояние')
plt.show()

# -=-=-=-=-=-=-=-=-= Прикрепление дендограмм к теплокарте
# При помощи атрибута add_axes создается новый объект figure задается позиция осей X и Y, ширина и высота дендограммы.
fig = plt.figure(figsize=(8,8), facecolor='white')
axd = fig.add_axes([0.09, 0.1, 0.2, 0.6])
row_dendr = dendrogram(row_clusters, orientation='right')
# для matplotlib >= v1.5.1, использовать orientation='left'

# Далее переупорядочить данные в нашей исходной таблице данных DataFrame согласно меткам кластеризации, к которым можно обратиться из
# объекта дендограммы, являющегося по существу словарем Python, по ключу leaves.
df_rowclust = df.ix[row_dendr['leaves'][::-1]]

# Далее строим теплокарту из переупорядоченной таблицы данных DataFrame и расположим ее прямо напротив дендограммы:
axm = fig.add_axes([0.23, 0.1, 0.6, 0.6])
cax = axm.matshow(df_rowclust, interpolation='nearest', cmap='hot_r')

# Изменим визуализацию теплокарты, удалив деление осей и убрав линии сетки. Кроме того, добавить цветную полосу и назначить меткам
# делений соотвественно оси X и Y имена признаков и образцов.
axd.set_xticks([])
axd.set_yticks([])
for i in axd.spines.values():
    i.set_visible(False)
fig.colorbar(cax)
axm.set_xticklabels([''] + list(df_rowclust.columns))
axm.set_yticklabels([''] + list(df_rowclust.index))
plt.show()
# Порядок следования строк в теплокарте отражает кластеризацию образцов в дендограмме.

# -=-=-=-=-=-=-=-=-=-=-
# Агломеративная кластеризация в scikit-learn

# Мы использовали агломеративную кластеризацию, используя библиотеку SciPy. В библиотеке scikit-learn тоже имеется реализация агломеративной
# кластеризации - класс AgglomerativeClusering, позволяющий выбирать число кластеров, которое мы хотим получить.

# Установив параметр n_clusters равным 2, сгруппируем образцы в две группы с использованием того же самого метода полной связи на основе
# евклидовой метрики расстояния.

from sklearn.cluster import AgglomerativeClustering
ac = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')

labels = ac.fit_predict(X)
print('Метки кластеров: %s' % labels)

# Тут видно, что образцы ID_0, ID_3, ID_4 были назначены кластеру 0, образцы ID_1, ID_2 были назначены кластеру 1, что согласуется с результатами,
# которые мы можем наблюдать на дендограмме.
