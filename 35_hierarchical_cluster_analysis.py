# Альтернативный подход к кластеризации к кластеризации на основе прототипов: иерархическая кластеризация.
# Одно из преимуществ иерархической кластеризации состоит в том, что она позволяет строить дендограммы (древовидные визуализации бинарной
# иерархической кластеризации), которые способны помочь интерпретировать результаты путем создания содержательных таксономий.
# Другое преимущество состоит в том, что нам не нужно предварительно указывать число кластеров.

# Существует 2 осоновных подхода к иерархической кластеризации: агломеративный (объединительный) и дивизивный (разделяющий).
# В дивизионной иерархической кластеризации мы начинаем с единственного кластера, который охватывает все ваши образцы, и мы итеративно
# расщепляем на более мелкие кластеры, пока там не останется по одному образцу в каждом кластере.

# Сейчас мы сконцентрируемся на агломеративной кластеризации, в которой мы будем брать каждый образец, как отдельный кластер и объединять
# ближайшие пары кластеров, пока не останется всего один кластер.

# Агломеративная иерархическая кластеризация представлена двумя стандартными алгоритмами: методом одиночной связи (single linkage) или
# метод ближайших соседей, и методом полной связи (complete linkage) или метод дальнего соседа.

# Используя метод одиночной связи, для каждой пары кластеров мы вычисляем расстояния между самыми похожими членами и объединяем два
# кластера, для которых расстояние между самыми похожими членами наименьшее.

# Подход на основе полной связи подобен методу одиночной связи, но, вместо того, чтобы в каждой паре кластеров сравнивать самых похожих,
# для выполнения объединения мы сравниваем наиболее различающихся членов.

# -=-=-=-=-=-=-=-=-=-=-=- Метод агломеративной кластеризации на основе метода полной связи
# 1. Вычисляется матрица расстояний всех образцов
# 2. Представляется каждая точка данных, как одноэлементный кластер
# 3. Объединить два ближайших кластера, основываясь на расстоянии наиболее различающихся (дальних членов)
# 4. Обновить матрицу расстояний
# 5. Повторять шаги 2-3, пока не останется единственный кластер

# Вычисление матрицы расстояний
# Сгенерируем набор тестовых данных со случайными значениями:
# Строки - разные наблюдения, столбцы разные признаки этих наблюдений

import pandas as pd
import numpy as np
np.random.seed(123)
variables = ['X', 'Y', 'Z']
labels = ['ID_0','ID_1','ID_2','ID_3','ID_4']
X = np.random.random_sample([5,3])*10
df = pd.DataFrame(X, columns=variables, index=labels)
print(df[:10])

# -=-=-=-=-=-=- Вычисление матрицы расстояний с использованием spatial.distance библиотеки SciPy
from scipy.spatial.distance import pdist, squareform
row_dist = pd.DataFrame(squareform(pdist(df, metric='euclidean')), columns=labels, index=labels)
print(row_dist)

# -=-=-=-=-=-=- Применение алгоритма агломеративной кластеризации на основе метода полной связи, воспользовавшись функцией linkage,
# которая возвращает так называемую матрицу связи
from scipy.cluster.hierarchy import linkage
# help(linkage)
# Неправильный подход в этом случае - это использовать квардратную матрицу расстояний
# Правильный подход использовать сжатая матрица расстояний или используется матрица входных образцов

# Неправильный подход, используется квадратная матрица расстояний
row_clusters=linkage(row_dist, method='complete', metric='euclidean')
# Правильный подход, используется сжатая матрица расстояний
row_clusters=linkage(pdist(df, metric='euclidean'), method='complete')

# Правильный подход, используется матрица входных образцов
row_clusters=linkage(df.values, metric='euclidean', method='complete')

# Преобразование таблицы в DataFrame
pd.DataFrame(row_clusters, columns=['метка строки 1', 'метка строки 2', 'расстояние', 'число элементов в кластере'],
             index=['кластер %d' % (i+1) for i in range(row_clusters.shape[0])])
# Матрица связей состоит из нескольких строк, где каждая строка представляет собой одно объединение.
# Первый и второй столбцы обозначают наиболее различающихся членов в каждом кластере, третий столбец говорит о расстоянии между этими членами.
# Последний столбец возвращает число членов в каждом кластере.

# Теперь можно визуализировать результаты в виде дендограммы
from scipy.cluster.hierarchy import dendrogram
# делаем дендограмму черной (часть 1)
# from scipy.cluster.hierarchy import set_link_color_palette
# set_link_color_palette(['black'])
row_dendr = dendrogram(row_clusters, labels=labels)
# row_dendr = dendrogram(row_clusters, labels=labels,
# делаем дендограмму черно (часть 2)
# color_threshold=np.inf)

import matplotlib.pyplot as plt
plt.tight_layout()
plt.ylabel('Евклидово расстояние')
plt.show()
