# Альтернативный подход к кластеризации к кластеризации на основе прототипов: иерархическая кластеризация.
# Одно из преимуществ иерархической кластеризации состоит в том, что она позволяет строить дендограммы (древовидные визуализации бинарной
# иерархической кластеризации), которые способны помочь интерпретировать результаты путем создания содержательных таксономий.
# Другое преимущество состоит в том, что нам не нужно предварительно указывать число кластеров.

# Существует 2 осоновных подхода к иерархической кластеризации: агломеративный (объединительный) и дивизивный (разделяющий).
# В дивизионной иерархической кластеризации мы начинаем с единственного кластера, который охватывает все ваши образцы, и мы итеративно
# расщепляем на более мелкие кластеры, пока там не останется по одному образцу в каждом кластере.

# Сейчас мы сконцентрируемся на агломеративной кластеризации, в которой мы будем брать каждый образец, как отдельный кластер и объединять
# ближайшие пары кластеров, пока не останется всего один кластер.

# Агломеративная иерархическая кластеризация представлена двумя стандартными алгоритмами: методом одиночной связи (single linkage) или
# метод ближайших соседей, и методом полной связи (complete linkage) или метод дальнего соседа.

# Используя метод одиночной связи, для каждой пары кластеров мы вычисляем расстояния между самыми похожими членами и объединяем два
# кластера, для которых расстояние между самыми похожими членами наименьшее.

# Подход на основе полной связи подобен методу одиночной связи, но, вместо того, чтобы в каждой паре кластеров сравнивать самых похожих,
# для выполнения объединения мы сравниваем наиболее различающихся членов.

# -=-=-=-=-=-=-=-=-=-=-=- Метод агломеративной кластеризации на основе метода полной связи
# 1. Вычисляется матрица расстояний всех образцов
# 2. Представляется каждая точка данных, как одноэлементный кластер
# 3. Объединить два ближайших кластера, основываясь на расстоянии наиболее различающихся (дальних членов)
# 4. Обновить матрицу расстояний
# 5. Повторять шаги 2-3, пока не останется единственный кластер

# Вычисление матрицы расстояний
# Сгенерируем набор тестовых данных со случайными значениями:
# Строки - разные наблюдения, столбцы разные признаки этих наблюдений

import pandas as pd
import numpy as np
np.random.seed(123)
variables = ['X', 'Y', 'Z']
labels = ['ID_0','ID_1','ID_2','ID_3','ID_4']
X = np.random.random_sample([5,3])*10
df = pd.DataFrame(X, columns=variables, index=labels)
print(df[:10])
