# Глубокое обучение - набор алгоритмов и методов, разработанных для тренировки искусственных нейронных сетей, состоящих из множества слоев.
#
# -=-=-=-=-=-=- Моделирование сложных функций искусственными нейронными сетями
#
# Алгоритмы глубокого обучения используются для создания из немаркированных данных детекторов признаков (feature detectors), которые
# применяются для предварительной тренировки глубоких нейронных сетей - нейронных сетей, скомпанованных из многих слоев.
#
# Архитектура однослойной нейронной сети на примере алгоритма ADALINE:
#
# Элемент смещения и входные значения -> весовые коэффициенты -> функция чистого входа -> функция активации -> единичная ступеньчатая функция
# -> предсказанные метки класса
#
# До этого мы реализовали алгоритм ADALINE для выполнения задачи бинарной классификации и применили алгоритм оптимизации на основе градиентоного
# спуска, чтобы извлечь весовые коэффициенты модели.
# При этом веса обновляются следующим образом: w = w + дельта(w), где дельта(w) - шаг в направлении антиградиента. Другими словами, мы вычисляем
# градиант на всех тренировочных данных и обновляем веса, делаю шаг в противоположно направлении.
#
# Для того, чтобы определить оптимальные веса, мы оптимизировали целевую функцию, которую определили как функцию стоимости на основе суммы
# квадратичных ошибок (SSE).
#
# Кроме того, мы умножали градиент на темп обучения, коэффициент, который мы выбрали, чтобы сбалансировать скорость обучения против риска
# непопадания по глобальному минимуму функции стоимости.
#
# Многослойная нейронная сеть с прямым распространением сигналов, этот тип также называется многослойный персептрон.
#
# Мы можем добавить в алгоритм многослойного персептрона несколько скрытых слоев. Но градиенты ошибок, которые мы вычисляем позже методом
# обратного распространения ошибки, стали бы по мере добавления новых слоев во все возрастающей степени уменьшаться. Эта проблема исчезновения
# градиента еще более усложняет извлечение модели, именно поэтому были придуманы алгоритмы глубокого обучения.
#
# -=-=-=-=-=-=- Активация нейронной сети методом прямого распространения сигнала
#
# Опишем процесс прямого распротранения сигналов для вычисления выхода модели многослойного персептрона (MLP), резюмируем процедуру обучения MLP
# следующими тремя шагами:
# 1. начиная с входного слоя, распространим по сети вперед образы (сигналы) тренировочных данных для генерирования данных
# 2. основываясь на выходе сети, расситать ошибку, подлежащую минимизации, с использованием функции стоимости
# 3. распространить ошибку назад, найти ее производную относительно каждого веса в сети и обновить модель
#
# В конце, после того, как шаги были выполнены для двух и более эпох и были извлечены веса MLP, мы используем прямое распространение сигналов,
# чтобы вычислить выход сети, и применяем пороговую функцию, чтобы получить метки классов.
#
# Термин "прямое распространение сигналов" обозначает, что каждый слой служит входом в следующий слой без циклов, в отличие от рекуррентных
# нейронных сетей.

import os
import struct
import numpy as np

def load_mnist(path, kind='train'):
    print('Загрузить данные Mnist из пути Path\n')
    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)
    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)

    with open(labels_path, 'rb') as lbpath:
        magic, n = struct.unpack('>II', lbpath.read(8))
        labels = np.fromfile(lbpath, dtype = np.uint8)

    with open(images_path, 'rb') as imgpath:
        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))
        images = np.fromfile(imgpath, dtype = np.uint8).reshape(len(labels), 784)

    return images, labels

X_train , y_train = load_mnist('/Users/grigorev-ee/Work/AnacondaProjects/My_projects/ml_w_python/data/mnist/', kind = 'train')
