# Глубокое обучение - набор алгоритмов и методов, разработанных для тренировки искусственных нейронных сетей, состоящих из множества слоев.
#
# -=-=-=-=-=-=- Моделирование сложных функций искусственными нейронными сетями
#
# Алгоритмы глубокого обучения используются для создания из немаркированных данных детекторов признаков (feature detectors), которые
# применяются для предварительной тренировки глубоких нейронных сетей - нейронных сетей, скомпанованных из многих слоев.
#
# Архитектура однослойной нейронной сети на примере алгоритма ADALINE:
#
# Элемент смещения и входные значения -> весовые коэффициенты -> функция чистого входа -> функция активации -> единичная ступеньчатая функция
# -> предсказанные метки класса
#
# До этого мы реализовали алгоритм ADALINE для выполнения задачи бинарной классификации и применили алгоритм оптимизации на основе градиентоного
# спуска, чтобы извлечь весовые коэффициенты модели.
# При этом веса обновляются следующим образом: w = w + дельта(w), где дельта(w) - шаг в направлении антиградиента. Другими словами, мы вычисляем
# градиант на всех тренировочных данных и обновляем веса, делаю шаг в противоположно направлении.
#
# Для того, чтобы определить оптимальные веса, мы оптимизировали целевую функцию, которую определили как функцию стоимости на основе суммы
# квадратичных ошибок (SSE).
#
# Кроме того, мы умножали градиент на темп обучения, коэффициент, который мы выбрали, чтобы сбалансировать скорость обучения против риска
# непопадания по глобальному минимуму функции стоимости.
#
# Многослойная нейронная сеть с прямым распространением сигналов, этот тип также называется многослойный персептрон.
#
# Мы можем добавить в алгоритм многослойного персептрона несколько скрытых слоев. Но градиенты ошибок, которые мы вычисляем позже методом
# обратного распространения ошибки, стали бы по мере добавления новых слоев во все возрастающей степени уменьшаться. Эта проблема исчезновения
# градиента еще более усложняет извлечение модели, именно поэтому были придуманы алгоритмы глубокого обучения.
#
# -=-=-=-=-=-=- Активация нейронной сети методом прямого распространения сигнала
#
# Опишем процесс прямого распротранения сигналов для вычисления выхода модели многослойного персептрона (MLP), резюмируем процедуру обучения MLP
# следующими тремя шагами:
# 1. начиная с входного слоя, распространим по сети вперед образы (сигналы) тренировочных данных для генерирования данных
# 2. основываясь на выходе сети, расситать ошибку, подлежащую минимизации, с использованием функции стоимости
# 3. распространить ошибку назад, найти ее производную относительно каждого веса в сети и обновить модель
#
# В конце, после того, как шаги были выполнены для двух и более эпох и были извлечены веса MLP, мы используем прямое распространение сигналов,
# чтобы вычислить выход сети, и применяем пороговую функцию, чтобы получить метки классов.
#
# Термин "прямое распространение сигналов" обозначает, что каждый слой служит входом в следующий слой без циклов, в отличие от рекуррентных
# нейронных сетей.

import os
import struct
import numpy as np

def load_mnist(path, kind='train'):
    print('Загрузить данные Mnist из пути Path\n')
    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)
    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)

    with open(labels_path, 'rb') as lbpath:
        magic, n = struct.unpack('>II', lbpath.read(8))
        labels = np.fromfile(lbpath, dtype = np.uint8)

    with open(images_path, 'rb') as imgpath:
        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))
        images = np.fromfile(imgpath, dtype = np.uint8).reshape(len(labels), 784)

    return images, labels

X_train , y_train = load_mnist('/Users/grigorev-ee/Work/AnacondaProjects/My_projects/ml_w_python/data/mnist/', kind = 'train')

print('Тренировка - строки: %d, столбцы: %d' % (X_train.shape[0], X_train.shape[1]))

X_test , y_test = load_mnist('/Users/grigorev-ee/Work/AnacondaProjects/My_projects/ml_w_python/data/mnist/', kind = 't10k')
print('Тестирование - строки: %d, столбцы: %d' % (X_test.shape[0], X_test.shape[1]))
# load_mnist возвращает 2 массива, первый, это n x m мерный массив NumPy, где n - число образцов, m - число признаков.
# Тренировочный набор состоит из 60 000 тренировочных цифр, тестовый набор содержит 10 000 образцов.
# Изображения в наборе данных MNIST представляют собой растры размеров 28 х 28 пикселей, в котором каждый пиксел представлен значением
# интенсивности полутоновой шкалы. Мы разворачиваем 28 х 28 пиксели в одномерные векторы-строки, соответствующие строкам в нашем массиве
# изображений (784 пиксела в расчете на строку или изображение). Второй массив labels содержит соответствующую целевую переменную,
# метки классов рукописных чисел.

# При помощи 2-х этих строк мы считываем число - описание формата файла или фалового протокола, а также число элементов (n) из
# файлового буфера и только потом методом fromfile считываем последующие байты в массив NumPy.
# Значение параметра '>II' имеет две части, первая > - обратный порядое байтов, I - беззнаковое целое число.
# magic, n = struct.unpack('>II', lbpath.read(8))
# labels = np.fromfile(lbpath, dtype = np.uint8)

# Чтобы получить представление о том, как выглядят изображения в MNIST, покажем примеры цифр 0-9 в наглядном виде после приведения
# 784-пиксельных векторов из нашей матрицы признаков в исходные изображения в формате 28 х 28, которые можно вывести на экран при помощи
# функции imshow библиотеки matplotlib.

import matplotlib.pyplot as plt
fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)
ax = ax.flatten()
for i in range(10):
    img = X_train[y_train==i][0].reshape(28,28)
    ax[i].imshow(img, cmap='Greys', interpolation='nearest')
ax[0].set_xticks([])
ax[0].set_yticks([])
plt.tight_layout()
plt.show()

# В дополнении к этому стоит также построить график с повторными примерами одной и той же цифры, чтобы подчеркнуть, насколько в действительности
# эти примеры отличаются и дополняют друг друга.
fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)
ax = ax.flatten()
for i in range(25):
    img = X_train[y_train==7][i].reshape(28,28)
    ax[i].imshow(img, cmap='Greys', interpolation='nearest')
ax[0].set_xticks([])
ax[0].set_yticks([])
plt.tight_layout()
plt.show()

# Чтобы сохранить данные в csv формате, чтобы использовать их в программах, которые не поддерживают специального байтового формата, необходимо:
# np.savetxt('train_img.csv', X_train, fmt='%i', delimniter=',')
# np.savetxt('train_labels.csv', y_train, fmt='%i', delimniter=',')
# np.savetxt('test_img.csv', X_test, fmt='%i', delimniter=',')
# np.savetxt('test_labels.csv', y_test, fmt='%i', delimniter=',')

# Чтобы зыгрузить их назад в питон необходимо выполнить следующий код:
# X_train = np.genfromtxt('train_img.csv', dtype=int, delimiter=',')
# y_train = np.genfromtxt('train_labels.csv', dtype=int, delimiter=',')
# X_test = np.genfromtxt('test_img.csv', dtype=int, delimiter=',')
# y_test = np.genfromtxt('test_labels.csv', dtype=int, delimiter=',')
