#-=-=-=-=-=-=-=-=- Погружения слов

# В википедии погружение, или векторное представление слов (word embedding) определяется как общее название различных методов языкового
# моделирования и обучения признаков, применяемых в обработке естественных языков (ОЕЯ, англ. NLP), когда слова или фразы из словаря
# отображаются на векторы вещественных чисел.
#
# Погружение слов – это способ преобразовать текстовое представление слов в числовые векторы, допускающие анализ стандартными алгоритмами
# машинного обучения, принимающими на входе числа.
#
# В главе 1 мы уже встречались с одним видом погружения слов – унитарным кодированием. Это самый простой подход к погружению.
# Напомним, что унитарным кодом слова будет вектор, число элементов которого равно размеру словаря, такой, что элемент, соответствующий
# данному слову, равен 1, а все остальные 0.
#
# Основная проблема унитарного кодирования в том, что нет никакого способа представить сходство слов. В любом заданном корпусе текстов
# мы ожидаем, что между словами «кошка» и «собака» или «нож» и «вилка» есть какое­то сходство. Сходство векторов вычисляется с помощью
# скалярного произведения, т. е. суммы произведений соответственных элементов.
#
# В случае унитарного кодирования скалярное произведение любых двух слов равно нулю.
# Для преодоления ограничений унитарного кодирования сообщество ОЕЯ заимствовало из информационного поиска (ИП) идею
# векторизации текста с использованием документа в качестве контекста. Здесь стоит отметить такие подходы,
# как TF­-IDF (https://en.wikipedia.org/wiki/Tf%E2%80%93idf), латентно-семантический анализ (ЛСА)
# (https://en.wikipedia.org/wiki/Latent_semantic_ analysis) и тематическое моделирование (https://en.wikipedia.org/ wiki/Topic_model).
#
# Но эти представления улавливают несколько иную, документо­центрическую, идею семантического сходства.

# Рассмотрим две формы погружения слов, GloVe и word2vec, известные под общим названием «распределенное представление слов».

# Мы также узнаем о способах порождения собственных погружений в программе на Keras, а равно о том, как использовать и настраивать
# предобученные модели на основе word2vec и GloVe.

# Будут рассмотрены следующие темы:
#  построение различных распределенных представлений слов в контексте;
#  построение моделей на основе погружений для решения таких задач ОЕЯ, как грамматический разбор предложения и анализ эмоциональной окраски.

# Распределенные представления
# Распределенное представление – это попытка уловить смысл слова путем рассмотрения его связей с другими словами в контексте.
# Эта идея сформулирована в следующем высказывании Дж. Р. Фир- та (J. R. Firth)
# (см. статью Andrew M. Dai, Christopher Olah, Quoc V. Le «Document Embedding with Paragraph Vectors», arXiv:1507.07998, 2015), лингвиста,
# который первым выдвинул ее:

# Мы узнаем слово по компании, с которой оно дружит.
# Рассмотрим такие два предложения:
# Париж – столица Франции.
# Берлин – столица Германии.

# Даже если вы совсем не знаете географию (или русский язык), все равно нетрудно сообразить, что пары слов (Париж, Берлин) и (Франция, Германия)
# как­то связаны и что между соответственными словами связи одинаковы, т. е.
# Париж : Франция :: Берлин : Германия

# Следовательно, задача распределенного представления – найти такую общую функцию φ преобразования слова в соответствующий ему вектор,
# что справедливы соотношения следующего вида:
# φ («Париж») – φ («Франция») ≈ φ («Берлин») – φ («Германия»)

# Иными словами, цель распределенного представления – преобразовать слова в векторы, так чтобы сходство векторов коррелировало с семантическим
# сходством слов.
#
# В следующих разделах мы рассмотрим два наиболее известных погружения слов: word2vec и GloVe.

#-=-=-=-=-=-=-=-=- word2vec

# Группа моделей word2vec была разработана в 2013 году группой исследователей Google под руководством Томаша Миколова (Tomas Mikolov).
# Модели обучаются без учителя на большом корпусе текстов и порождают векторное пространство слов.
# Размерность пространства погружения word2vec обычно меньше размерности пространства погружения для унитарного кодирования,
# которая равна размеру словаря. Кроме того, это пространство погружения плотнее разреженного погружения при унитарном кодировании.

# Существуют две архитектуры word2vec:
# 1. непрерывный мешок слов (Continuous Bag Of Words, CBOW);
# 2. skip­ граммы.

# В архитектуре CBOW модель предсказывает текущее слово, если известно окно окружающих его слов.
# Кроме того, порядок контекстных слов не влияет на предсказание (это допущение модели мешка слов).
# В архитектуре skip­ грамм модель предсказывает окружающие слова по известному центральному слову.
# Согласно заявлению авторов, CBOW быстрее, но skip­граммы лучше предсказывают редкие слова.

# Интересно отметить, что хотя word2vec создает погружения, используемые в моделях глубокого обучения ОЕЯ, оба варианта word2vec,
# которые мы будем обсуждать и которые снискали наибольший успех и признание, являются мелкими нейронными сетями.

# -=-=-=-=-=- Модель CBOW
# Теперь рассмотрим модель CBOW из семейства word2vec. Напомним, что она предсказывает слово по известным контекстным словам.
# Таким образом, для первого кортежа из примера ниже CBOW должна предсказать слово love, зная контекстные слова I и green:

# ([I, green], love) ([love, eggs], green) ([green, and], eggs) ...

# Как и модель skip­грамм, модель CBOW представляет собой классификатор, который получает на входе контекстные слова и предсказывает целевое слово.
# Но архитектура проще, чем в моде- ли skip­грамм. Входными данными модели являются идентифика- торы контекстных слов.
# Они поступают на вход слоя погружения, веса которого инициализируются небольшими случайными значениями.
# Этот слой преобразует каждый идентификатор в вектор размера embed_size.
# Следовательно, каждая строка входного контекста преобразуется в матрицу размера (2*window_size, embed_size).
# Затем матрица подается на вход слоя lambda, который вычисляет среднее по всем погружениям.
# Полученная величина передается плотному слою, который создает плотный вектор размера vocab_ size для каждой строки.
# В качестве функции активации в плотном слое используется softmax, которая возвращает вероятность, равную максимальному элементу
# выходного вектора. Идентификатор с максимальной вероятностью соответствует целевому слову.

# Ниже приведен код модели на Keras. Мы снова предполагаем, что размер словаря равен 5000, размер выходного пространства погружения 300,
# размер контекстного окна 1. Сначала импортируем пакеты и инициализируем переменные:
from keras.models import Sequential
from keras.layers.core import Dense, Lambda
from keras.layers.embeddings import Embedding
import keras.backend as K

vocab_size = 5000
embed_size = 300
window_size = 1

# Затем строим последовательную модель, в которую включаем слой погружения с весами, инициализированными малыми случайными значениями.
# Отметим, что длина входа input_length этого слоя равна числу контекстных слов. Каждое контекстное слово подается на вход слоя,
# и веса обновляются в процессе обратного распространения. На выходе слоя получается матрица погружений контекстных слов,
# которая усредняется в один вектор (на каждую строку входа) слоем lambda.
# Наконец, плотный слой преобразует строки в плотный вектор размера vocab_size. Целевым словом будет то, для которого вероятность
# идентификатора в плотном выход- ном векторе максимальна.

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embed_size, embeddings_initializer='glorot_uniform', input_length=window_size*2))
model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))
model.add(Dense(vocab_size, kernel_initializer='glorot_uniform',activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer="adam")

# В качестве функции потерь здесь используется categorical_ crossentropy – типичный выбор для случая, когда категорий две или больше
# (в нашем примере vocab_size).
