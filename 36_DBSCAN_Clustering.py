# Локализация областей высокой плотности алгоритмом DBSCAN

# Плотностная пространственная кластеризация приложений с присутсвием шума (density-based spatial clustering of applications with noise, DBSCAN)
# Понятие плотности в DBSCAN определяется, как число точек внутри указанного радиуса epsilon.

# В DBSCAN кажому образцу (точке) назначается специальная метка, пир этом используются следующие критерии:
# 1. точка рассматривается, как корневая если, по меньшей мере, указанное число окрестных точек (MinPts) попадает в пределы указанного радиуса eps
# 2. граничная точка - это точка, имеющая соседей меньше, чем MinPts в пределах epsilon, но лежащая в пределах радиуса epsilon корневой точки
# 3. все остальные точки, которые не являются ни корневыми, ни граничными точками, рассматриваются, как шумовые точки

# После маркировки точек на корневые, граничные и шумовые, алгоритм можно резюмировать в двух простых шагах:
# 1. сформировать для каждой корневой точки отдельный кластер, либо связанную группу корневых точек (корневые точки являются связанными, если
# они расположены не дальше, чем epsilon)
# 2. назначить каждую граничную точку кластеру соответствующей корневой точки.

# Одно из основных премуществ использования алгоритма DBSCAN состоит в точ, что он не делает допущения о сферичной форме кластеров, как в алгоритмие
# к-средних. Кроме того, алгоритм DBSCAN отличается от алгоритмов к-средних и иерархической кластеризации тем, что он с необходимостью не
# назначает кажду точку кластеру и одновременно способен удалять шумовые точки.

# Создадим набор данных в форме полумесяца, чтобы сравнить кластеризацию по методу к-средних, иерархическую и DBSCAN:

from sklearn.datasets import make_moons
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)

import matplotlib.pyplot as plt
plt.scatter(X[:,0], X[:,1])
plt.show()

# Начнем с применения алгоритма к-средних и кластеризации с полной связью, чтобы увидеть, сможет ли один из алгоритмов кластеризации,
# обсуждавшийся ранее успешно идентифицировать фигуры полумесяца, как отдельные кластеры.

f, (ax1, ax2) = plt.subplots(1,2, figsize=(8,3))

from sklearn.cluster import KMeans
km = KMeans(n_clusters=2, random_state=0)
y_km = km.fit_predict(X)

ax1.scatter(X[y_km==0,0],
            X[y_km==0,1],
            c='lightblue',
            marker='o',
            s=40,
            label='Кластер №1')

ax1.scatter(X[y_km==1,0],
            X[y_km==1,1],
            c='red',
            marker='s',
            s=40,
            label='Кластер №2')

ax1.set_title('Кластеризация методом К-средних')

# -=-=-=-=-=-

from sklearn.cluster import AgglomerativeClustering
ac = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')

y_ac = ac.fit_predict(X)

ax2.scatter(X[y_ac==0,0],
            X[y_ac==0,1],
            c='lightblue',
            marker='o',
            s=40,
            label='Кластер №1')

ax2.scatter(X[y_ac==1,0],
            X[y_ac==1,1],
            c='red',
            marker='s',
            s=40,
            label='Кластер №2')

ax2.set_title('Агломеративная кластеризация')
plt.legend()
plt.show()

# -=-=-=-=-=-

from sklearn.cluster import DBSCAN
db = DBSCAN(eps=0.2, min_samples=5, metric='euclidean',)

y_db = db.fit_predict(X)

plt.scatter(X[y_db==0,0],
            X[y_db==0,1],
            c='lightblue',
            marker='o',
            s=40,
            label='Кластер №1')

plt.scatter(X[y_db==1,0],
            X[y_db==1,1],
            c='red',
            marker='s',
            s=40,
            label='Кластер №2')

ax2.set_title('Агломеративная кластеризация')
plt.legend()
plt.show()

# Недостатки алгоритма DBSCAN.
# С растущим числом признаков в наборе данных - при заданном фиксированном размере тренировочного набора - отрицательный эффект проклятия
# размерности увеличивается. Эта проблема в особенности проявляется, если мы используем евклидову метрику расстояния.
# В добавок в алгоритме DBSCAN есть 2 гиперпараметра, которые должны быть оптимизированы с целью получения хороших результатов.
# Нахождение хорошей комбинации может быть проблематичным, если в наборе данных разницы в плотностях относительно большие.

# Мы разобрали 3 основных алгоритма кластеризации, одна существуют более продвинутые алгоритмы, графовые алгоритмы кластеризации.
# Наиболее яркий пример из семейства данных алгоритмов, это алгоритмы спектральной кластеризации. Несмотря на то, что существует множество
# вариантов реализации данных алгоритмов, всех их объединяет то, что они используют собственные векторы из матрицы подобия для получения
# кластерных связей.
